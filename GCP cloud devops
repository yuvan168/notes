# Google Cloud Platform (GCP) - Complete Notes & Theory

## Table of Contents
1. [GCP Fundamentals](#gcp-fundamentals)
2. [Core Services](#core-services)
3. [Compute Services](#compute-services)
4. [Networking](#networking)
5. [Databases](#databases)
6. [Storage Services](#storage-services)
7. [Security & IAM](#security--iam)
8. [DevOps & Deployment](#devops--deployment)
9. [Monitoring & Logging](#monitoring--logging)
10. [Best Practices](#best-practices)

---

## GCP Fundamentals

### What is GCP?
```
Google Cloud Platform is a suite of cloud computing services that runs on the 
same infrastructure Google uses internally.

Key Characteristics:
├─ Global infrastructure (34+ regions, 105+ zones)
├─ Pay-as-you-go pricing model
├─ Strong data analytics capabilities (BigQuery, Dataflow)
├─ Industry-leading machine learning services
├─ Advanced security and compliance features
└─ Integration with Google Workspace and Firebase
```

### Theory: Why Cloud Computing?

**Before Cloud (Traditional Data Centers):**
```
Problems:
├─ High upfront capital cost (buy servers, storage)
├─ Long procurement time (order, ship, setup)
├─ Under-utilization (buy for peak, idle during off-peak)
├─ Maintenance burden (physical servers break down)
├─ Scaling difficult (add more hardware)
├─ No geographic distribution
└─ Complex disaster recovery
```

**With Cloud Computing:**
```
Benefits:
├─ Elasticity: Scale up/down automatically based on demand
├─ Flexibility: Pay only for what you use
├─ Agility: Deploy globally in minutes
├─ Reliability: Redundancy, automatic failover
├─ Security: Managed by cloud provider
├─ Maintenance: Provider manages hardware
└─ Focus: You manage applications, not infrastructure
```

**The Shared Responsibility Model:**
```
Google (GCP) Responsible For:
├─ Physical security (data centers, locks, guards)
├─ Infrastructure security (server hardware, network)
├─ Virtualization security (hypervisors, isolation)
├─ Patches and updates (OS level)
└─ Compliance certifications (SOC 2, ISO, HIPAA)

You (Customer) Responsible For:
├─ Application security (code vulnerabilities, authentication)
├─ Data security (encryption keys, access control)
├─ Configuration (firewall rules, IAM policies)
├─ Data privacy (PII handling, compliance)
└─ Identity management (who can access what)

This responsibility scales based on service type:
IaaS (Compute Engine): You manage more
PaaS (App Engine): You manage less
SaaS (Gmail, Docs): You manage least
```

**Key Principle: Infrastructure as Code**
```
Why IaC?
├─ Reproducibility: Same infrastructure every time
├─ Version control: Track changes, rollback if needed
├─ Consistency: No manual mistakes
├─ Testing: Validate before deploying
├─ Automation: Deploy at scale automatically
└─ Documentation: Code documents infrastructure

Tools:
├─ Terraform: Multi-cloud, most popular
├─ CloudFormation: AWS-only
├─ Deployment Manager: GCP-native
└─ Helm: Kubernetes-specific
```

### GCP Hierarchy
```
Organization (Top level)
├─ Folders (Grouping projects)
│  └─ Projects
│     ├─ Resources (VMs, databases, storage)
│     ├─ IAM policies
│     └─ Billing account
```

### Project Structure
```
A GCP Project is the fundamental organizing entity:
├─ Contains all resources (compute, storage, networking)
├─ Has unique PROJECT_ID (immutable)
├─ Linked to billing account
├─ Has associated APIs that must be enabled
├─ Uses service accounts for automation
└─ Example: my-project-123456
```

**Key Concepts Behind Projects:**

**1. Resource Hierarchy (Organizational Structure)**
```
Why hierarchy?
├─ Security: Different permissions at different levels
├─ Billing: Track costs by department/project
├─ Governance: Enforce policies across organization
└─ Management: Organize teams and workloads

Example:
Organization (root)
├─ Production Folder
│  ├─ Project: prod-app (billing: $5K/month)
│  ├─ Project: prod-analytics (billing: $3K/month)
│  └─ IAM Policy: only eng team can access
├─ Development Folder
│  ├─ Project: dev-app
│  ├─ Project: dev-analytics
│  └─ IAM Policy: all engineers can access
└─ Finance Folder
   └─ Project: cost-analysis
      └─ IAM Policy: only finance team can access
```

**2. Project Isolation**
```
Each project is completely isolated:
├─ Resources don't communicate by default
├─ IAM policies separate
├─ Billing separate
├─ Quotas separate
├─ Data never crosses project boundaries

This isolation prevents:
├─ Accidental deletion (delete project A, project B unaffected)
├─ Security breach spreading (compromise project A, project B safe)
├─ Resource contention (heavy load in project A doesn't affect project B)
└─ Cost spillover (project A costs don't appear in project B)

Cross-project communication (when needed):
├─ Explicit IAM role granted across projects
├─ Service accounts with cross-project permissions
├─ Shared VPC (for GCP resources)
└─ API calls (authenticated with credentials)
```

**3. APIs and Quotas**
```
Every GCP service has an API that must be enabled:
├─ Compute Engine API
├─ Cloud Storage API
├─ Cloud SQL API
├─ Cloud Run API
└─ BigQuery API

Why?
├─ Avoid accidental provisioning (must explicitly enable)
├─ Separate billing for each API
├─ Track usage per API
└─ Security (only enable services you use)

Quotas (limits per project):
├─ Rate limits (requests per second)
├─ Concurrency limits (simultaneous operations)
├─ Resource limits (number of instances, storage size)
└─ Region-specific limits (vary by location)

Example quotas:
├─ Cloud Run: 50M invocations/day (free tier)
├─ Cloud Build: 120 minutes/day (free tier)
├─ Compute Engine: 12 CPU cores per region (varies)
└─ BigQuery: No hard limit (pay per query)
```

### Regions & Zones
```
Region: Geographic location (e.g., us-central1)
├─ Multiple zones for high availability
├─ Lower latency within region
└─ Data stays within region (compliance)

Zone: Isolated location within region
├─ Independent infrastructure
├─ Provides fault isolation
└─ Example: us-central1-a, us-central1-b

Why Multi-zone?
├─ Availability: If one zone fails, others operate
├─ Load distribution: Spread traffic
├─ Disaster recovery: Automatic failover
└─ Cost: Usually negligible difference
```

**Theory: Availability & Reliability**

**1. Single Zone vs Multi-Zone Deployment**
```
Single Zone (HA = 99.5%):
┌──────────────────────────────┐
│      Zone us-central1-a       │
├──────────────────────────────┤
│  ┌──────────┐  ┌──────────┐  │
│  │ Instance │  │ Instance │  │
│  │    1     │  │    2     │  │
│  └──────────┘  └──────────┘  │
│                               │
│  Single building, power,      │
│  network in this zone         │
└──────────────────────────────┘

Risk:
├─ Power outage in zone → all instances down
├─ Network issue in zone → all instances unreachable
├─ Google infrastructure maintenance → downtime
└─ 99.5% uptime = 3.7 hours downtime per year
```

```
Multi-Zone (HA = 99.95%):
┌──────────────────┐    ┌──────────────────┐
│  Zone a          │    │  Zone b          │
├──────────────────┤    ├──────────────────┤
│ ┌──────────┐     │    │ ┌──────────┐     │
│ │Instance 1│     │    │ │Instance 2│     │
│ └──────────┘     │    │ └──────────┘     │
│                  │    │                  │
│ Different        │    │ Different        │
│ building, power, │    │ building, power, │
│ network          │    │ network          │
└──────────────────┘    └──────────────────┘
         ↓                       ↓
      Load Balancer (distributes traffic)

If zone a fails:
├─ Zone a instances down
├─ Load balancer routes to zone b
├─ Users see no interruption
└─ 99.95% uptime = 22 minutes downtime per year
```

**2. High Availability (HA) vs Disaster Recovery (DR)**
```
High Availability (HA):
├─ Goal: Minimize downtime (seconds)
├─ Scope: Within region (low latency)
├─ Cost: Moderate (need redundant instances)
├─ Recovery: Automatic (load balancer fails over)
└─ Example: Multi-zone deployment, managed instance groups

Disaster Recovery (DR):
├─ Goal: Recover from regional failure (hours)
├─ Scope: Across regions (higher latency)
├─ Cost: Higher (replica infrastructure in another region)
├─ Recovery: Manual or planned failover
└─ Example: Cross-region Cloud SQL replica, GCS multi-region bucket
```

**3. Data Residency & Compliance**
```
Key Principle: Data locality for compliance

GDPR (Europe):
├─ Personal data must be stored in EU
├─ Solution: Create resources in europe-west1
├─ GCP enforces: Data doesn't leave region

HIPAA (Healthcare):
├─ Protected health info must stay in US
├─ Solution: Use us-central1, us-east1
├─ GCP enforces: Encryption at rest, audit logs

Example regulation impact:
Company A (Global):
├─ User data: europe-west1 (EU users)
├─ User data: us-central1 (US users)
├─ Analytics: Can query with BigQuery Federated Queries
├─ Cost: Multiple databases instead of 1 global
└─ Complexity: Data doesn't cross borders, but can analyze together
```

**Key Points:**
```
✓ Region = geographic area (us-central1 = Iowa)
✓ Zone = data center within region (us-central1-a)
✓ Always use multi-zone for production
✓ Cross-region for disaster recovery (backup)
✓ Data residency requirements determine region choice
✓ Latency increases with distance (East Coast to West Coast = 50ms)
✓ Costs vary by region (asia-northeast1 often more expensive)
```

---

## Database & Data Services - Theory & Concepts

### Understanding CAP Theorem

**CAP Theorem: Pick Any Two**
```
A distributed system can guarantee only 2 of 3 properties:

┌─────────────┐    ┌──────────────┐    ┌──────────────┐
│  Consistency│    │ Availability │    │  Partition   │
│             │    │              │    │ Tolerance   │
│ All reads   │    │ System always│    │ Handles      │
│ return      │    │ responds     │    │ network      │
│ latest data │    │ (no errors)  │    │ failures     │
└─────────────┘    └──────────────┘    └──────────────┘
       ↓                  ↓                    ↓
       └──────────────────┼────────────────────┘
          Pick any 2 of 3

Examples:

Traditional RDBMS (SQL):
├─ Consistency: ✓ (ACID transactions)
├─ Availability: ✗ (fails on partitions)
├─ Partition Tolerance: ✗ (requires sync replication)
└─ Example: Cloud SQL (single region)

NoSQL Key-Value (Firestore):
├─ Consistency: ✓ (strong consistency within region)
├─ Availability: ✓ (returns cached data)
├─ Partition Tolerance: ✗ (reads may be stale)
└─ Example: Firestore with multi-region

Distributed (Bigtable):
├─ Consistency: ✗ (eventual consistency)
├─ Availability: ✓ (always responds)
├─ Partition Tolerance: ✓ (handles failures)
└─ Example: Bigtable, DynamoDB
```

### Database Selection Matrix

**How to Choose Database:**

```
Question Tree:

1. Structured SQL queries with JOINs?
   YES → Cloud SQL (MySQL, PostgreSQL)
   NO  → Continue

2. Document data (JSON objects, nested)?
   YES → Firestore
   NO  → Continue

3. Massive scale (100GB+ partitioned)?
   YES → Bigtable or BigQuery
   NO  → Continue

4. Complex transactions across services?
   YES → Cloud Spanner (if global, needs strong consistency)
   NO  → Continue

5. Real-time analytics on big data?
   YES → BigQuery
   NO  → Continue

6. Time-series metrics (billions of points)?
   YES → Bigtable
   NO  → Continue

Database Comparison:

                    Cloud SQL   Firestore   Spanner   Bigtable   BigQuery
────────────────────────────────────────────────────────────────────────
Consistency         Strong      Strong      Strong    Eventual   N/A
Throughput          10K TPS     10K TPS     10K TPS   1M TPS     N/A
Data Size           10TB        1PB         5PB       100PB      1000PB
Latency             10-100ms    10-100ms    10-100ms  1-10ms     Seconds
Query Language      SQL         Firestore   SQL       No         SQL
Transactions        Yes         Yes         Yes       No         No
Cost/month          $100-$5K    $0-$5K      $5K+      $500-$10K  $6-$3K/TB
Best For            Traditional Documents   Global    Time-series Analytics
```

### Consistency Models Explained

**Strong Consistency vs Eventual Consistency:**

```
Strong Consistency (Cloud SQL, Spanner):
┌─────────────────────────────────────────┐
│ Write "John's balance = $100"           │
│ ├─ Wait for all replicas to confirm     │
│ └─ Return success                       │
├─────────────────────────────────────────┤
│ Read "John's balance"                   │
│ ├─ All nodes return $100 (same value)   │
│ └─ No stale data possible               │
└─────────────────────────────────────────┘

Guarantee: Every read sees latest write
Cost: Higher latency (wait for replication)
Use for: Money, medical records, critical data

Eventual Consistency (Bigtable, DynamoDB):
┌─────────────────────────────────────────┐
│ Write "John's balance = $100" to Node A │
│ ├─ Node A: $100 ✓                       │
│ ├─ Node B: $50 (old) - still replicating
│ └─ Return success immediately           │
├─────────────────────────────────────────┤
│ Read from Node B (unlucky)              │
│ └─ Get $50 (stale, will converge soon)  │
│                                         │
│ After 100ms, all nodes converge:        │
│ ├─ Node A: $100 ✓                       │
│ ├─ Node B: $100 ✓ (eventually)          │
│ └─ Node C: $100 ✓ (eventually)          │
└─────────────────────────────────────────┘

Guarantee: All replicas eventually match
Cost: Lower latency (return immediately)
Use for: Social media, analytics, caches

Trade-off:
├─ Strong consistency = Slower but safer
├─ Eventual consistency = Faster but risky
└─ Choose based on data importance
```

### Replication Strategies

**Master-Slave Replication (Cloud SQL):**
```
┌────────────────┐
│ Primary (Master)│
│ Accepts writes │
└────────┬────────┘
         │ Replicates writes
         ↓
┌────────────────┐  ┌────────────────┐
│Replica 1       │  │Replica 2       │
│Read-only       │  │Read-only       │
└────────────────┘  └────────────────┘

Advantages:
├─ Simple (one source of truth)
├─ Strong consistency (writes to master)
└─ Cheap (only master has full I/O)

Disadvantages:
├─ Master is SPOF (single point of failure)
├─ Failover requires manual intervention
└─ Replicas lag behind master
```

**Multi-Master Replication (Cloud Spanner):**
```
┌────────────────┐      ┌────────────────┐
│Master 1        │      │Master 2        │
│Accepts writes  │      │Accepts writes  │
└────────┬───────┘      └────────┬───────┘
         │  Replicates changes   │
         └──────────┬────────────┘
                    ↓
         ┌────────────────────┐
         │ Consensus protocol  │
         │ (agrees on order)  │
         └────────────────────┘

Advantages:
├─ No SPOF (any master can serve writes)
├─ Automatic failover
├─ Global write availability
└─ Strong consistency maintained

Disadvantages:
├─ Complex (distributed consensus)
├─ Higher cost
└─ Conflict resolution needed
```

---

## Networking & Security - Theory & Concepts

### VPC (Virtual Private Cloud) Fundamentals

**What is a VPC?**
```
A VPC is your private network in the cloud - completely isolated from others

Physical Reality (Before VPC):
┌──────────────────────────────────┐
│       Google Physical Network      │
│ ┌──────────────┐  ┌─────────────┐ │
│ │Customer A    │  │Customer B   │ │
│ │192.168.1.0/24   10.0.0.0/8    │ │
│ │Same physical │  │IP conflict! │ │
│ │network!      │  │Danger!      │ │
│ └──────────────┘  └─────────────┘ │
└──────────────────────────────────┘

VPC Solution:
┌──────────────────────────────────────┐
│       Google Physical Network         │
│ ┌─────────────────┐  ┌────────────┐  │
│ │VPC A            │  │VPC B       │  │
│ │192.168.1.0/24   │  │10.0.0.0/8  │  │
│ │Completely      │  │Completely │  │
│ │Isolated        │  │Isolated   │  │
│ │(Logical)       │  │(Logical)  │  │
│ └─────────────────┘  └────────────┘  │
│                                      │
│ Even with same IP, no conflict       │
│ Cannot communicate by default        │
└──────────────────────────────────────┘
```

**Subnets (Sub-networks):**
```
VPC can have multiple subnets in different regions

VPC "my-vpc"
├─ Subnet "us-central" (us-central1)
│  ├─ IP range: 10.1.0.0/24 (256 IPs)
│  └─ Resources: 10.1.0.2, 10.1.0.3, ...
├─ Subnet "europe-west" (europe-west1)
│  ├─ IP range: 10.2.0.0/24 (256 IPs)
│  └─ Resources: 10.2.0.2, 10.2.0.3, ...
└─ Subnet "asia-east" (asia-east1)
   ├─ IP range: 10.3.0.0/24 (256 IPs)
   └─ Resources: 10.3.0.2, 10.3.0.3, ...

Note: Subnets span zones within region
├─ us-central-1a and us-central1-b
└─ Both use same subnet IP range
```

**Key Points:**
```
✓ VPC = Your private network (isolated from others)
✓ Subnets = Smaller networks within VPC
✓ All resources in same VPC can communicate (by default)
✓ Resources in different VPCs cannot communicate (by default)
✓ Resources in same subnet = same zone? No! (subnets span zones)
✓ Private IP = internal only (10.0.0.0, 192.168.0.0, 172.16.0.0)
✓ External IP = internet-facing (assigned by Google)
```

### Compute Engine (IaaS)
```
Virtual Machines in GCP

Use Cases:
├─ Lift and shift applications
├─ Custom software requirements
├─ Full control over OS and middleware
└─ High-performance computing

Instance Types:
├─ General Purpose (n1, n2, e2) - Balanced CPU/Memory
├─ Memory-Optimized (m1, m2) - High RAM
├─ Compute-Optimized (c2) - High CPU
└─ Accelerators (GPU, TPU) - ML workloads

Pricing Model:
├─ Per-second billing (minimum 1 minute)
├─ Sustained use discounts (automatic, up to 30%)
├─ Committed use discounts (CUDs, up to 70% for 3-year)
├─ Preemptible VMs (up to 80% cheaper, can be interrupted)
└─ Sole-tenant nodes (dedicated physical server)

Key Features:
├─ Snapshots: Point-in-time backup of disk
├─ Images: Reusable VM templates
├─ Instance groups: Auto-scaling groups
├─ Startup scripts: Initialization commands
└─ Metadata: Pass configuration to instances

Networking:
├─ Multiple network interfaces per VM
├─ Custom firewall rules
├─ Internal and external IP addresses
├─ Cloud NAT for private VM internet access
└─ VPN and Interconnect for on-prem connectivity
```

**Theory: Virtual Machines & Virtualization**

**1. What is Virtualization?**
```
Physical Server (Before Virtualization):
┌─────────────────────────────────┐
│     OS: Ubuntu Server            │
├─────────────────────────────────┤
│  App 1    │  App 2    │  App 3   │
├─────────────────────────────────┤
│ CPU: 64 cores, RAM: 256GB        │
│ Disk: 10TB (all in one machine)  │
└─────────────────────────────────┘

Problems:
├─ All apps share resources
├─ One bad app crashes everything
├─ Can't isolate workloads
├─ Difficult to scale
├─ Underutilized (paying for full server)

Virtual Machines (With Virtualization):
┌─────────────────────────────────────────────┐
│        Hypervisor (KVM, Xen)                │
├──────────────┬──────────────┬───────────────┤
│  VM 1        │  VM 2        │  VM 3         │
│  OS: Ubuntu  │  OS: Windows │  OS: Ubuntu   │
│  App 1       │  App 2       │  App 3        │
│  CPU: 8      │  CPU: 16     │  CPU: 8       │
│  RAM: 32GB   │  RAM: 64GB   │  RAM: 64GB    │
│  Disk: 100GB │  Disk: 500GB │  Disk: 200GB  │
├──────────────┼──────────────┼───────────────┤
│  Physical Resources (shared but isolated)    │
│  CPU: 64 cores  │  RAM: 256GB  │  Disk: 10TB │
└─────────────────────────────────────────────┘

Benefits:
├─ Isolation: VM 1 crash doesn't affect VM 2
├─ Efficient: Each VM sized for its needs
├─ Flexibility: Different OS per VM
├─ Scalability: Add more VMs as needed
└─ Cost: Pay for what you use
```

**2. Compute Engine Pricing Deep Dive**
```
Scenario: Running 100 n1-standard-4 VMs for 1 year

Option A: Per-minute pricing
├─ Per-minute cost: $0.19
├─ Minutes per month: 24 * 60 * 30 = 43,200
├─ Cost per VM per month: $0.19 * 43,200 = $8,208
├─ Cost for 100 VMs: $820,800
└─ Total yearly: $9,849,600 ❌ VERY EXPENSIVE

Option B: Sustained Use Discount (automatic)
├─ First 25% of month (normal price): $2,052 * 25% = $513
├─ Next 25% (10% discount): $2,052 * 25% * 0.9 = $462
├─ Next 25% (20% discount): $2,052 * 25% * 0.8 = $410
├─ Last 25% (30% discount): $2,052 * 25% * 0.7 = $360
├─ Cost per VM per month: $1,745 (automatically)
├─ Cost for 100 VMs: $174,500
└─ Total yearly: $2,094,000 (78% savings!)

Option C: 3-Year CUD (Commitment)
├─ Commit to 100 VMs for 3 years
├─ Gets 70% discount
├─ Cost per VM per month: $2,052 * 0.3 = $616
├─ Cost for 100 VMs: $61,600
└─ Total yearly: $739,200 (93% savings!)

Lesson:
├─ Always use sustained use discounts (automatic)
├─ Consider CUDs for predictable workloads
├─ Preemptible for non-critical (80% discount)
└─ Combination of all = optimal cost
```

**3. Capacity Commitment & Planning**
```
Why Capacity?
├─ CPU/GPU/Memory is shared across customers
├─ During high demand, might not have capacity for your VM
├─ Solution: Commitment ensures capacity reserved

With Commitment:
├─ Reserve capacity for 1 or 3 years
├─ Pay in advance (monthly or annually)
├─ Get significant discount (25-70%)
├─ Guaranteed availability

Without Commitment:
├─ On-demand pricing (100% cost)
├─ Best effort capacity (might be unavailable)
├─ Highly flexible (can delete anytime)
└─ Pay per second (no waste)

Use Cases:
├─ Production workloads → Use CUD (predictable cost)
├─ Dev/test environments → Pay per second (flexible)
├─ Sudden spike → On-demand + preemptible (temporary)
```

**Key Points:**
```
✓ VMs = full OS + app = maximum control
✓ Best for: Legacy apps, custom configurations
✓ Worst for: Simple APIs, batch jobs
✓ Always right-size: Monitor actual usage
✓ Use MIG (managed instance groups) for auto-scaling
✓ Snapshots for backup (incremental, efficient)
✓ Startup scripts for initialization (automation)
✓ Sustained use discounts are automatic + free
✓ CUDs worth it only for stable 1-3 year workloads
```

### App Engine (PaaS)
```
Managed application hosting for web and mobile backends

Standard Environment:
├─ Fully managed, serverless
├─ Automatic scaling (0 to thousands)
├─ Built-in services (sessions, caching, auth)
├─ Runtimes: Node.js, Python, Java, Go, Ruby, PHP
├─ Code pushes: gcloud app deploy
└─ No infrastructure management

Flexible Environment:
├─ More control than Standard
├─ Custom runtime support (Docker containers)
├─ Larger instances available
├─ Longer startup times
└─ Still managed (no OS patching needed)

Benefits:
├─ No server management
├─ Automatic scaling
├─ Built-in security
├─ Integrated with GCP services
└─ Simple deployment (gcloud app deploy)

Considerations:
├─ Vendor lock-in (App Engine APIs)
├─ Cold starts (5-30 seconds from 0 to 1)
├─ Limited customization
└─ Scaling to zero costs nothing (good for dev/test)

Example Deployment:
```
app.yaml:
runtime: nodejs18
env: standard

Deploy:
$ gcloud app deploy
```

**Theory: Platform as a Service (PaaS)**

**1. Abstraction Levels**
```
IaaS vs PaaS vs SaaS (The Cloud Hierarchy)

Traditional (On-Premises):
┌─────────────────────────────────┐
│ Application (your code)         │ You manage
│ Middleware (web server, DB)     │ You manage
│ OS (Linux, Windows)             │ You manage
│ Virtualization                  │ You manage
│ Hardware (servers, networking)  │ You manage
│ Facilities (building, power)    │ You manage
└─────────────────────────────────┘

IaaS (Compute Engine):
┌─────────────────────────────────┐
│ Application (your code)         │ You manage
│ Middleware (web server, DB)     │ You manage
│ OS (Linux, Windows)             │ You manage
│ Virtualization                  │ Google manages
│ Hardware (servers, networking)  │ Google manages
│ Facilities (building, power)    │ Google manages
└─────────────────────────────────┘
Problem: Still need to manage OS, middleware, patches

PaaS (App Engine):
┌─────────────────────────────────┐
│ Application (your code)         │ You manage
│ Middleware (web server)         │ Google manages
│ OS (Linux)                      │ Google manages
│ Virtualization                  │ Google manages
│ Hardware (servers, networking)  │ Google manages
│ Facilities (building, power)    │ Google manages
└─────────────────────────────────┘
Benefit: You only worry about code

SaaS (Gmail, Docs):
┌─────────────────────────────────┐
│ Application (email, docs)       │ Google manages
│ Middleware                      │ Google manages
│ OS                              │ Google manages
│ Virtualization                  │ Google manages
│ Hardware                        │ Google manages
│ Facilities                      │ Google manages
└─────────────────────────────────┘
Benefit: Zero infrastructure management, just use it
```

**2. App Engine Standard vs Flexible**
```
App Engine Standard (Lightweight):
┌──────────────────────────────────────┐
│  Your Application Code               │
├──────────────────────────────────────┤
│  Standard Runtime                    │
│  (optimized for quick startup)       │
├──────────────────────────────────────┤
│  GCP Managed                         │
│  • Automatic scaling (0 to N)        │
│  • Cold start: 5-10 seconds          │
│  • Only supported runtimes           │
│  • Lightweight (think serverless)    │
└──────────────────────────────────────┘

Characteristics:
├─ Cold start (first request slow)
├─ Fixed runtimes (no custom OS)
├─ Limited disk I/O (can't write to disk)
├─ Request timeout: 60 seconds
├─ Memory: 128MB to 2.3GB
└─ Ideal: Stateless web apps

App Engine Flexible (Full Control):
┌──────────────────────────────────────┐
│  Your Application Code               │
├──────────────────────────────────────┤
│  Custom Dockerfile (any image)       │
├──────────────────────────────────────┤
│  Docker Container                    │
│  (runs on Compute Engine)            │
├──────────────────────────────────────┤
│  GCP Managed                         │
│  • Automatic scaling                 │
│  • Cold start: 30-60 seconds         │
│  • Custom dependencies               │
└──────────────────────────────────────┘

Characteristics:
├─ Longer startup (full container)
├─ Custom runtimes (bring any Dockerfile)
├─ Full disk access
├─ Request timeout: 60 minutes
├─ Memory: 512MB to 8GB
├─ Ideal: Complex apps, legacy code

Choose Standard if:
├─ Stateless web APIs
├─ Quick startup important
├─ Standard runtime sufficient
└─ Cost sensitive

Choose Flexible if:
├─ Custom dependencies needed
├─ Need more resources (memory, CPU)
├─ Longer request timeout needed
└─ Container-based deployment preferred
```

**3. Cost Comparison**
```
Scenario: Simple API handling 100 requests/day

App Engine Standard:
├─ 100 requests/day = 8 min billing/day
├─ Instances can scale to 0 = $0 when idle
├─ Instance hour: $0.05
├─ Monthly cost: $0.02 (yes, very cheap!)

Compute Engine:
├─ 1 x n1-standard-1 running 24/7
├─ Cost: $0.048/hour
├─ Monthly cost: $34.56

Lesson:
├─ Stateless workloads → App Engine (massive savings)
├─ Consistent load → Compute Engine (simpler management)
├─ Variable load → App Engine (pay per request)
```

**Key Points:**
```
✓ App Engine = Managed web hosting (no ops needed)
✓ Standard = Lightweight, cheap, fast startup
✓ Flexible = Full control, custom runtime, slower startup
✓ Scales to zero (pay for nothing when idle)
✓ Perfect for: Web APIs, microservices, prototypes
✓ Not suitable for: Long-running batch jobs, custom OS needs
✓ Vendor lock-in: But rapid development benefit
```

### Cloud Run (Serverless Containers)
```
Run containerized applications without managing infrastructure

Key Features:
├─ Fully serverless and managed
├─ Any language (use Docker containers)
├─ Automatic scaling (0 to thousands)
├─ Per-second billing
├─ Scales to zero (no cost when idle)
├─ Stateless (designed for functions)
└─ HTTP requests trigger execution

How it Works:
1. Build Docker image
2. Push to Artifact Registry
3. Deploy: gcloud run deploy --image gcr.io/project/image
4. Get HTTPS endpoint
5. Invoke via HTTP requests

Scaling:
├─ Minimum instances: Keep at least N warm (cost)
├─ Maximum instances: Prevent runaway costs
├─ Concurrency: Max requests per instance (default 80)
└─ Auto-scaling: Based on incoming traffic

Use Cases:
├─ REST APIs
├─ Webhooks
├─ Scheduled jobs (with Cloud Scheduler)
├─ Event processors (Pub/Sub, Cloud Events)
└─ Data processing pipelines

Pricing:
├─ Per-request (2M free requests/month)
├─ CPU time (GHz-seconds)
├─ Allocation: Memory (128MB to 8GB)
└─ Network egress
```

**Theory: Serverless & Functions as a Service (FaaS)**

**1. What is Serverless?**
```
Myth: "Serverless" means no servers
Reality: Servers exist, but you don't manage them

Key Principle:
├─ You provide: Code (Docker container or function)
├─ Platform provides: Infrastructure (auto-scaling, management)
├─ You pay: Only for execution time
└─ You manage: Nothing (no servers, networking, patches)

Before Serverless (Traditional):
You provision server
    ↓
You configure it
    ↓
You scale it manually
    ↓
You monitor it
    ↓
You patch it
    ↓
You pay 24/7 (even when idle)

With Serverless:
You upload code
    ↓
Platform automatically scales
    ↓
You pay only for actual execution
    ↓
Platform handles everything else
```

**2. Stateless Architecture**
```
Why Cloud Run requires stateless code?

Stateful (Bad for Cloud Run):
┌─────────────────────────────────┐
│  Instance 1                     │
│  ├─ User123: session data      │  If instance dies:
│  ├─ User456: session data      │  └─ Data is lost!
│  └─ Cache: 100MB               │
└─────────────────────────────────┘

If Cloud Run scales to 0:
├─ Instance dies
├─ Session data lost
├─ User logged out
├─ Cache cleared
└─ Bad user experience

Stateless (Good for Cloud Run):
┌─────────────────────────────────┐
│  Instance 1                     │ Instance dies?
│  ├─ No session data (stored     │ No problem!
│  │   in database)              │ New instance
│  ├─ No cache (in Redis)        │ serves request
│  └─ No local files             │ Same data available
└─────────────────────────────────┘

All state in external services:
├─ Sessions: Cloud Datastore/Firestore
├─ Cache: Redis/Memcached
├─ Files: Cloud Storage
├─ Data: Cloud SQL/BigQuery
└─ Instance: Completely ephemeral

Benefit:
├─ Scales to zero (no waste)
├─ Fault tolerant (instance failure = replaced)
├─ Distributed (can run millions of copies)
```

**3. Cold Starts & Optimization**
```
Cold Start Timeline:
T=0:00   Request arrives
T=0:00   Platform: Need new instance
T=0:50   Docker image: Downloaded, extracted, started
T=1:20   Application: Startup code executed
T=1:50   Request: Processed
T=2:00   Response: Sent to user

Total latency: 2 seconds (user noticed!)

Warm Start Timeline:
T=0:00   Request arrives
T=0:01   Instance ready (was already running)
T=0:01   Request: Processed
T=0:02   Response: Sent

Total latency: 100ms (user happy!)

Optimization strategies:
├─ Minimize container size (faster download)
├─ Pre-initialize expensive resources (before handling requests)
├─ Use faster language (Go > Python)
├─ Keep minimum instances warm (cost tradeoff)
└─ Cache aggressively (avoid slow operations)
```

**4. Billing Model**
```
Cloud Run is extremely cheap for variable workloads:

Scenario: API handling 10,000 requests/day

Traditional Server:
├─ n1-standard-1 running 24/7
├─ Cost: $0.048/hour
├─ Monthly: $34.56
├─ Utilization: 0.4% (mostly idle)
└─ Wasted cost: $34.40/month

Cloud Run:
├─ 10,000 req/day
├─ Avg 200ms per request = 2,000 seconds/day
├─ Per second: $0.00002400
├─ Memory: 256MB (included in price)
├─ Monthly: 0.06 seconds * 30 days * $0.00002400 = $0.04
└─ Wasted cost: $0

Savings: 99.9% cheaper for bursty workloads!

But if traffic 24/7:
├─ Cloud Run: $0.00002400 * 86,400 * 30 = $62.21
├─ Traditional: $34.56
└─ Compute Engine cheaper for steady state
```

**Key Points:**
```
✓ Serverless = Managed infrastructure, pay per execution
✓ Perfect for: APIs, webhooks, event handlers, batch jobs
✓ Not suitable for: Always-on services, interactive sessions
✓ Must be stateless (all state in external storage)
✓ Cold starts acceptable for: Async, background jobs
✓ Cold starts problematic for: Interactive APIs, user-facing
✓ Keep minimum instances warm if cold start unacceptable
✓ Scales to zero = Best cost for unpredictable traffic
```

### Cloud Functions (Serverless Functions)
```
Event-driven serverless functions (lightweight)

Triggered By:
├─ Cloud Pub/Sub
├─ Cloud Storage (bucket events)
├─ Firestore (document changes)
├─ Cloud Tasks
├─ HTTP requests
├─ BigQuery
└─ Custom events

Supported Runtimes:
├─ Node.js (18, 20)
├─ Python (3.9, 3.10, 3.11)
├─ Go (1.19, 1.20)
├─ Java (17)
├─ .NET (.NET 6, 7)
└─ Ruby (3.2)

Memory & Timeout:
├─ Memory: 128MB to 16GB
├─ CPU allocated: Based on memory (up to 4 vCPU)
├─ Timeout: 60 seconds (HTTP), 540 seconds (background)
└─ Execution time: Billed per 100ms

Cloud Functions vs Cloud Run:
├─ Functions: Event-driven, simple, short-lived
├─ Run: More flexible, longer running, containerized
├─ Choose Functions for: Simple handlers, events
├─ Choose Run for: Complex apps, longer tasks
└─ Note: Cloud Functions 2nd Gen runs on Cloud Run
```

### GKE (Google Kubernetes Engine)
```
Managed Kubernetes service

Components:
├─ Control plane (fully managed, no cost)
├─ Worker nodes (Compute Engine instances)
├─ Container runtime: containerd
├─ Networking: Pod networking, service discovery
└─ Storage: Persistent volumes, snapshots

Cluster Types:
├─ Standard cluster: Typical applications
├─ Autopilot cluster: Fully managed (GCP manages nodes)
└─ Regional cluster: Nodes across multiple zones (HA)

Features:
├─ Automatic node updates
├─ Automatic cluster upgrades
├─ Built-in networking and security
├─ Integration with GCP services
├─ Horizontal Pod Autoscaling (HPA)
├─ Vertical Pod Autoscaling (VPA)
└─ Cluster autoscaling (add/remove nodes)

Networking:
├─ VPC-native (Alias IP ranges)
├─ Network policies (firewall for pods)
├─ Ingress (external load balancing)
├─ Service types: ClusterIP, NodePort, LoadBalancer
└─ DNS: Automatic service discovery

Storage in GKE:
├─ Persistent Disk (block storage)
├─ Filestore (NFS)
├─ GCS (object storage, not for pods directly)
├─ ConfigMaps (configuration)
└─ Secrets (sensitive data)

Security:
├─ Workload Identity (secure service account access)
├─ Pod Security Policy/Standards
├─ Network policies (ingress/egress)
├─ RBAC (role-based access control)
└─ Binary Authorization (verified container images)
```

---

### Firewall & Network Security

**Firewall Rules (Stateful):**
```
GCP Firewalls are STATEFUL:
├─ Track connections
├─ Allow response traffic automatically
└─ More efficient than stateless

Example (E-commerce):
Request OUT to Stripe API:
┌─ Outbound rule needed: Allow TCP 443 to 1.2.3.4
├─ Connection established
└─ Response from Stripe automatically allowed
   (no inbound rule needed, tracked by stateful firewall)

Rule Components:
├─ Direction: Ingress (inbound) or Egress (outbound)
├─ Priority: 0-65534 (lower = higher priority, evaluated first)
├─ Action: ALLOW or DENY
├─ Source/Destination: IP ranges, service accounts, network tags
├─ Protocol: TCP, UDP, ICMP, etc.
├─ Port: Specific (443) or range (8000-9000)
└─ Targets: Which resources this rule applies to
```

**Default Firewall Behavior:**
```
By default:
├─ Ingress: ALL DENIED (except inter-VPC)
├─ Egress: ALL ALLOWED
└─ Result: Resources can talk to internet, but internet can't reach them

Common mistake:
├─ Create VM without firewall rule for HTTP
├─ VM has web server running
├─ External traffic doesn't reach it (blocked)
├─ "Why isn't my website accessible?"
└─ Solution: Add ingress rule for TCP:80
```

**Key Points:**
```
✓ Firewall rules protect network boundary (not OS firewall)
✓ Stateful = responses tracked automatically
✓ Default: Deny all ingress, allow all egress
✓ Network tags = easy way to target multiple resources
✓ Service accounts = identity-based rules (more flexible)
✓ Project-wide rules apply to all VPCs
✓ VPC-wide rules apply to all subnets/resources
```
```
Virtual network for GCP resources

Components:
├─ VPC Network: Isolated private network
├─ Subnets: IP ranges within VPC
├─ Routes: Traffic routing rules
├─ Firewall Rules: Allow/deny traffic
└─ Cloud NAT: Network Address Translation

Network Modes:
├─ Auto mode: Subnets created automatically in each region
├─ Custom mode: Manual subnet creation (recommended)
└─ Switching from auto to custom: Possible but one-way

Firewall Rules:
├─ Direction: Ingress (inbound) or Egress (outbound)
├─ Priority: 0-65534 (lower = higher priority)
├─ Action: ALLOW or DENY
├─ Conditions: Source/destination IPs, protocols, ports
├─ Targets: Network tags, service accounts
└─ Default: Deny all ingress, allow all egress

Example Firewall Rule:
```
Network: default
Direction: Ingress
Priority: 1000
Targets: tcp-servers (tag)
Source: 0.0.0.0/0
Port: tcp:8080
Action: ALLOW
```

Connectivity Options:
├─ Cloud VPN: Encrypted tunnel to on-premises (1.5 Gbps)
├─ Cloud Interconnect: Dedicated connection (10 Gbps+)
├─ Cloud NAT: Private VM internet access (outbound only)
└─ Private Google Access: Private VMs access Google services
```

### Load Balancing
```
Distribute traffic across multiple targets

Types of Load Balancers:

1. HTTP(S) Load Balancer (Layer 7)
   ├─ Global external load balancer
   ├─ URL-based routing
   ├─ Host-based routing
   ├─ Best for: Web applications, APIs
   └─ Auto-scaling based on CPU/memory

2. TCP/UDP Load Balancer (Layer 4)
   ├─ Regional or global
   ├─ Extreme performance (millions of requests)
   ├─ Best for: Non-HTTP protocols, real-time apps
   └─ Gaming, IoT, DNS

3. Internal Load Balancer
   ├─ Private IP address (RFC 1918)
   ├─ Only accessible within VPC
   ├─ Best for: Internal services
   └─ Microservices communication

4. Cloud CDN
   ├─ Content Delivery Network
   ├─ Caches content at Google POPs globally
   ├─ Reduces origin load and latency
   └─ Works with HTTPS Load Balancer

Health Checks:
├─ HTTP/HTTPS health checks
├─ TCP/SSL health checks
├─ Custom criteria (check interval, timeout, threshold)
├─ Unhealthy instances removed from traffic
└─ Automatic recovery when instance heals
```

### Cloud Armor
```
DDoS and WAF (Web Application Firewall)

Protection:
├─ Layer 7 (application) attacks
├─ Layer 4 (transport) attacks
├─ SQL injection
├─ Cross-site scripting (XSS)
├─ Volumetric attacks
└─ Bot traffic

Rules:
├─ Allow/Deny based on:
│  ├─ IP address/subnet
│  ├─ Geolocation
│  ├─ Request headers
│  ├─ Request methods
│  └─ Path patterns
└─ Adaptive protection (ML-based)

Example Policy:
```
1. Block all requests from country X
2. Allow only HTTPS
3. Rate limit: Max 100 req/min per IP
4. Block requests with SQL keywords in URI
5. Default: ALLOW
```
```

---

## Databases

### Cloud SQL
```
Managed relational database (MySQL, PostgreSQL, SQL Server)

Features:
├─ Fully managed (automated backups, patches)
├─ Multi-zone high availability
├─ Automatic failover
├─ Automated backups (configurable retention)
├─ Point-in-time recovery
├─ Read replicas (for scaling reads)
├─ SSL/TLS encryption in transit
└─ Encryption at rest

Instance Classes:
├─ db-f1-micro: Shared CPU (development)
├─ db-n1-standard-*: 1 to 96 vCPU (production)
├─ db-n1-memory-*: Memory-optimized
└─ db-highcpu-*: CPU-optimized

Backup Strategy:
├─ Automated backups: Daily (configurable time window)
├─ Retention: Up to 35 days
├─ Manual backups: On-demand
├─ Point-in-time restore: Up to 35 days
└─ Cost: First backup free, additional backups charged

Scaling:
├─ Vertical: Increase machine type (requires downtime)
├─ Horizontal: Read replicas (for reads only)
├─ Connection pooling: Cloud SQL Proxy or PgBouncer
└─ Monitoring: Query Insights for performance analysis

Connectivity:
├─ Cloud SQL Proxy: Secure connection from anywhere
├─ VPC connectors: From App Engine/Functions
├─ Public IP: Not recommended (use Cloud SQL Proxy)
└─ Private IP: Only accessible from within VPC

Disaster Recovery:
├─ HA configuration: Multi-zone failover
├─ Read replicas: Can be promoted to primary
├─ Cross-region replicas: For geo-redundancy
└─ Automated backup to GCS: For long-term retention
```

### Firestore (NoSQL)
```
Serverless, fully managed NoSQL document database

Data Model:
├─ Collections: Groups of documents
├─ Documents: Contain fields (key-value pairs)
├─ Fields: Can be strings, numbers, booleans, maps, arrays
├─ Subcollections: Collections within documents
└─ Automatic indexing: Most queries are fast

Modes:
├─ Native mode: Use Firestore directly (recommended)
└─ Datastore mode: Cloud Datastore compatibility

Benefits:
├─ Serverless (no capacity planning)
├─ Real-time synchronization (listeners)
├─ Offline support (sync when online)
├─ ACID transactions
├─ Automatic scaling
└─ Per-document pricing (cost predictable)

Queries:
├─ Simple: equality filters
├─ Compound: multiple conditions (requires index)
├─ Range: <, >, <=, >=
├─ Ordering: ORDER BY field
└─ Limits: LIMIT clause

Real-time Listeners:
```javascript
// Listen to changes in real-time
db.collection('users')
  .onSnapshot(querySnapshot => {
    querySnapshot.forEach(doc => {
      console.log(doc.id, doc.data());
    });
  });
```

Transactions:
```javascript
// ACID transaction: All or nothing
db.runTransaction(async (transaction) => {
  const doc = await transaction.get(userRef);
  transaction.update(userRef, { balance: balance - amount });
  transaction.update(accountRef, { total: total + amount });
});
```

Pricing:
├─ Read: $0.06 per 100K
├─ Write: $0.18 per 100K
├─ Delete: $0.02 per 100K
├─ Storage: ~$0.18 per GB/month
└─ Free tier: 50K reads/day, 20K writes/day

Security:
├─ Firebase Security Rules (query-level access control)
├─ Authentication: Firebase Auth
├─ Encryption at rest: Always encrypted
└─ Encryption in transit: TLS
```

### Cloud Spanner
```
Globally distributed, strongly consistent relational database

Key Feature: Distributed SQL
├─ Traditional SQL (ACID, joins, foreign keys)
├─ Global scale (multi-region)
├─ Strong consistency across regions
├─ Horizontal scalability
└─ No eventual consistency issues

Use Cases:
├─ Global applications needing strong consistency
├─ High-frequency trading systems
├─ Multi-region e-commerce
├─ Applications requiring complex queries
└─ When you need SQL + global scale

Instance Configuration:
├─ Multi-region: Replicated across regions
├─ Region: Single region (lower cost)
├─ Node count: Each node = 2TB storage, 10K QPS
└─ Pricing: Per node-hour

Scaling:
├─ Vertical: Add nodes to instance
├─ Horizontal: Database shards automatically
├─ No application changes needed for scaling
└─ Hot spots: Spanner detects and re-distributes

Replication:
├─ Synchronous: Commits wait for quorum (strong consistency)
├─ Asynchronous: Read-only replicas
└─ Trade-off: Latency vs consistency

When to Use:
├─ ✓ Global scale with strong consistency needed
├─ ✓ Complex queries (joins, aggregations)
├─ ✓ Traditional SQL patterns
├─ ✗ Simple key-value lookups (use Firestore)
├─ ✗ Cost-sensitive (expensive at scale)
```

### Bigtable
```
Wide-column, distributed database (massive scale)

Data Model:
├─ Tables: Collections of rows
├─ Rows: Identified by row key (like database primary key)
├─ Column families: Logical grouping of columns
├─ Columns: Within column families
├─ Timestamps: Version control on each cell
└─ Cells: Row × column × timestamp

Use Cases:
├─ Time-series data (IoT, metrics, logs)
├─ Analytics (HBase-compatible)
├─ Real-time bidding
├─ Recommendations
└─ Graph databases

Scale:
├─ Petabytes of data
├─ Millions of read/write requests per second
├─ Automatic sharding by row key
├─ No SQL (use HBase API, Dataflow, etc.)

Why Bigtable?
├─ Automatic scaling (no manual sharding)
├─ Low latency (milliseconds)
├─ High throughput
└─ Cost effective at massive scale

Row Key Design (Critical!):
```
Good: 'user#123#timestamp' (distributed writes)
Bad: 'timestamp#user#123' (hotspot at recent timestamps)

Why? Bigtable shards by row key prefix.
If all writes go to same prefix → bottleneck
```

Replication:
├─ Within region (automatic)
├─ Cross-region: Multi-cluster replication
└─ Async replication (eventual consistency)
```

### BigQuery
```
Data warehouse (analytics, not OLTP)

Characteristics:
├─ Columnar storage (optimized for analytics)
├─ SQL interface (standard SQL)
├─ Sub-second query performance (with caching)
├─ Automatic scaling (no capacity planning)
├─ Serverless (pay only for data scanned)
└─ Petabyte-scale queries

Pricing Model:
├─ Slot-based: Fixed cost, unlimited queries
│  ├─ Annual commitment: Best for stable workload
│  └─ Flex slots: Hourly billing
├─ On-demand: Pay per GB scanned ($6.25/TB)
│  └─ First 1TB/month free
└─ Streaming inserts: $0.05 per 200MB

When to Use:
├─ ✓ Analytics queries
├─ ✓ Data exploration
├─ ✓ Business intelligence
├─ ✓ ML model training
├─ ✗ Real-time operational queries (too expensive)
├─ ✗ OLTP (row-level access)

Datasets & Tables:
```sql
-- Create dataset
CREATE SCHEMA my_project.my_dataset;

-- Create table
CREATE TABLE my_project.my_dataset.users (
  user_id INT64,
  name STRING,
  created_at TIMESTAMP
);

-- Query
SELECT name, COUNT(*) as count
FROM my_project.my_dataset.users
GROUP BY name;
```

Optimization Tips:
├─ Partition tables (by date, usually)
├─ Cluster tables (by frequently filtered columns)
├─ Use approximate aggregation functions (cardinality)
├─ Cache results (query results cached for 24 hours)
└─ Avoid SELECT * (specify columns)

BigQuery ML:
```sql
-- Create linear regression model
CREATE MODEL my_project.my_dataset.revenue_model
OPTIONS(
  model_type='linear_reg'
) AS
SELECT amount as label, quantity, price
FROM sales_data;

-- Predict
SELECT * FROM ML.PREDICT(MODEL my_project.my_dataset.revenue_model, 
  (SELECT quantity, price FROM new_sales));
```
```

---

## Storage Services

### Cloud Storage (GCS)
```
Object storage (like S3)

Characteristics:
├─ Serverless (pay per GB stored + requests)
├─ Global (accessible from anywhere)
├─ High availability (99.99% SLA)
├─ Unlimited scalability
└─ Not for file system operations

Bucket Naming:
├─ Globally unique (across all GCP)
├─ 3-63 characters, lowercase
├─ Cannot contain periods (.)
└─ Example: my-company-data-2024

Storage Classes:
├─ Standard: Hot data, any access pattern
├─ Nearline: Warm data, accessed < 1/month (cheaper)
├─ Coldline: Cold data, accessed < 1/quarter (cheaper)
├─ Archive: Very cold, accessed < 1/year (cheapest, 12-month min)
└─ Note: Early deletion fees apply to cold classes

Lifecycle Management:
```
Set rule: After 30 days → Move to Nearline
Set rule: After 90 days → Move to Coldline
Set rule: After 1 year → Delete

Cost Benefit: Saves significantly on old data
```

Access Control:
├─ Bucket-level: Uniform (recommended)
├─ Object-level: Fine-grained (legacy)
├─ Signed URLs: Temporary access tokens
├─ Service accounts: Application authentication
└─ Cloud IAM: Role-based access

Example (Python):
```python
from google.cloud import storage

# Upload
client = storage.Client()
bucket = client.bucket('my-bucket')
blob = bucket.blob('path/to/file.txt')
blob.upload_from_filename('local-file.txt')

# Download
blob.download_to_filename('downloaded-file.txt')

# Delete
blob.delete()
```

Versioning:
├─ Enable per bucket
├─ Keep versions of objects
├─ Restore previous versions
└─ Cost: Additional storage per version
```

### Filestore (NFS)
```
Managed NFS (Network File System) storage

Use Cases:
├─ GKE persistent volumes
├─ Compute Engine shared filesystems
├─ Data sharing between instances
└─ Legacy NFS applications

Capacity:
├─ Basic: 1TB, 2.5GB/s throughput
├─ Standard: 2.5TB minimum, 40GB/s throughput
├─ High Scale: 100TB+ with premium throughput
└─ Auto-scaling: Grows as needed (High Scale)

Performance:
├─ Lower latency than Cloud Storage
├─ Better for frequent file access
├─ Mounted like local filesystem
└─ Data persistence guaranteed

GKE Integration:
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: filestore-pv
spec:
  capacity:
    storage: 1Ti
  accessModes:
    - ReadWriteMany
  nfs:
    server: 10.0.0.10  # Filestore IP
    path: "/vol1"
```
```

---

## Security & IAM

### Cloud IAM (Identity & Access Management) - Theory

**Core Concepts:**

**1. Identity (WHO)**
```
Different types of identities:

Google Account:
├─ Real person (email@gmail.com)
├─ Login with Google
└─ Used for: Manual operations, developers

Service Account:
├─ Virtual identity for applications
├─ No password (uses keys or Workload Identity)
├─ Example: my-app@my-project.iam.gserviceaccount.com
└─ Used for: Automation, applications, services

Cloud Identity User:
├─ Corporate email (managed by IT)
├─ Single sign-on (SSO)
├─ Example: user@company.com
└─ Used for: Enterprise deployments

External Identities (Workload Identity Federation):
├─ From other platforms (AWS, GitHub, etc.)
├─ OIDC token-based
└─ Used for: Cross-cloud deployments, CI/CD
```

**2. Role (WHAT)**
```
Role = Set of permissions (granular actions allowed)

Examples:
├─ roles/viewer: Only list resources, no modifications
├─ roles/editor: Modify resources
├─ roles/admin: Full control including IAM changes
├─ roles/storage.admin: Full Cloud Storage access
├─ roles/cloudsql.client: Connect to Cloud SQL only

Predefined Roles (Google-managed):
├─ Maintained by Google
├─ Updated automatically with new features
├─ Recommended approach
└─ Example: roles/compute.instanceAdmin

Custom Roles (User-managed):
├─ Define exact permissions needed
├─ More flexible but requires expertise
├─ Can become security risk if misconfigured
├─ Example: roles/custom.projectSpecificRole

Basic Roles (Legacy - NOT RECOMMENDED):
├─ Viewer: Read-only
├─ Editor: Read + Write
├─ Owner: Full access
├─ Too coarse-grained
├─ Security risk (Owner can delete everything)
└─ Phasing out (don't use for new projects)
```

**3. Resource (WHERE)**
```
Hierarchy of resources (inheritance):

Organization (root)
├─ All policies inherited to folders and projects
│
├─ Folder (accounting)
│  ├─ All policies inherited to projects
│  │
│  ├─ Project (accounting-prod)
│  │  ├─ All policies inherited to resources
│  │  │
│  │  ├─ Cloud SQL instance
│  │  ├─ Compute Engine VM
│  │  └─ Cloud Storage bucket
│  │
│  └─ Project (accounting-dev)
│
└─ Folder (engineering)
   └─ Project (engineering-prod)

Inheritance Example:
┌─ Organization policy: "Require MFA"
│  └─ Inherited by all folders
│     └─ Inherited by all projects
│        └─ Inherited by all resources
│           └─ Every user needs MFA everywhere
```

**4. Policy Binding (Gluing It Together)**
```
IAM Policy Binding = Identity + Role + Resource

Syntax: identity@domain → role ← resource

Example:
alice@company.com → roles/editor ← my-project

Interpretation:
├─ WHO: alice@company.com
├─ CAN DO: Everything in roles/editor
└─ ON: Resources in my-project

Multiple Bindings Example:
┌─ alice@company.com → roles/editor ← my-project
├─ bob@company.com → roles/viewer ← my-project
├─ ci-bot@my-project.iam → roles/compute.admin ← my-project
├─ alice@company.com → roles/viewer ← other-project
└─ Result: Alice can edit my-project, view other-project
          Bob can view my-project
          CI bot can manage Compute Engine in my-project
```

**Principle of Least Privilege:**
```
Grant minimum permissions needed to do job

Bad Practice:
├─ Alice needs to deploy app
├─ Give her: roles/owner (full access to everything)
├─ Risk: She accidentally deletes production database
└─ Impact: Entire service down

Good Practice:
├─ Alice needs to deploy app
├─ Give her: roles/gke.developer (deploy to GKE only)
├─ Risk: She can't accidentally delete database
└─ Impact: Only her deployments affected if something wrong

Benefits:
├─ Reduce blast radius (if account compromised)
├─ Easier auditing (see what each person does)
├─ Prevent accidental deletions
└─ More secure by default
```

**Audit Trail (Who did What When):**
```
Every IAM change is logged:
├─ WHO: alice@company.com
├─ WHAT: Granted roles/editor
├─ WHEN: 2024-12-24 10:00 UTC
├─ WHERE: my-project
├─ RESULT: Success

Google Cloud Audit Logs show:
├─ All IAM changes
├─ All resource access
├─ All deletions
└─ Retention: 90 days (400 days for some events)

Use for:
├─ Compliance (prove who accessed what)
├─ Investigation (who deleted prod database?)
├─ Security (detect unusual access patterns)
└─ Compliance audits (SOC 2, ISO, HIPAA)
```

**Key Points:**
```
✓ Identity = WHO (person or service)
✓ Role = WHAT (set of permissions)
✓ Resource = WHERE (project, folder, organization)
✓ Binding = Identity + Role + Resource
✓ Inheritance = Policies flow down hierarchy
✓ Least privilege = Grant minimum needed permissions
✓ Service accounts = Identities for applications (no passwords)
✓ Audit logs = Track all IAM changes for compliance
✓ Avoid basic roles (Viewer, Editor, Owner) in production
✓ Use predefined roles (maintained by Google)
```
```

### VPC Service Controls
```
Isolate resources within security perimeter

Concept: Fortress around resources

Service Perimeter:
├─ Define which services are accessible
├─ Restrict data egress
├─ Block unauthorized access
└─ Works with any device/location

Access Policy → Service Perimeter → Resources

Example Policy:
```
┌─── Security Perimeter ───┐
│ Cloud Storage             │ (only access within perimeter)
│ Firestore                 │
│ BigQuery                  │
└───────────────────────────┘
  ↓
Only allow access from:
├─ Specific VPC
├─ Cloud NAT IP range
└─ Office VPN

Deny access from:
├─ Public internet
├─ Unsecured networks
```

Dry-Run Mode:
├─ Test policy without enforcing it
├─ Logs denied requests
└─ Helps identify issues before enabling
```

### Cloud KMS
```
Key Management Service for encryption keys

Use Cases:
├─ Encrypt data at rest
├─ Envelope encryption (app manages data keys, KMS manages master key)
├─ Signing (digital signatures)
└─ Key management and rotation

Key Hierarchy:
```
Organization
├─ Key Ring (container for keys)
│  └─ Crypto Key (a key)
│     └─ Key Version (e.g., v1, v2, v3)
```

Encryption Example (Python):
```python
from google.cloud import kms

client = kms.KeyManagementServiceClient()

# Encrypt
plaintext = b"secret data"
response = client.encrypt(
    request={
        "name": key_name,
        "plaintext": plaintext,
    }
)
ciphertext = response.ciphertext

# Decrypt
response = client.decrypt(
    request={
        "name": key_name,
        "ciphertext": ciphertext,
    }
)
plaintext = response.plaintext
```

Key Rotation:
├─ Automatic rotation (recommended)
├─ Interval: Every 90 days (configurable)
├─ Versions: Old versions kept for decryption
└─ No application changes needed
```

---

## DevOps & Deployment

### Cloud Build
```
Fully managed CI/CD service

How it Works:
```
1. Commit code to Git repo (GitHub, Bitbucket, GitLab)
2. Cloud Build triggered (webhook)
3. Runs steps in cloudbuild.yaml
4. Build Docker image
5. Push to Artifact Registry
6. Deploy to production
```

cloudbuild.yaml:
```yaml
steps:
  # Build image
  - name: 'gcr.io/cloud-builders/docker'
    args: 
      - 'build'
      - '-t'
      - 'gcr.io/$PROJECT_ID/my-app:$SHORT_SHA'
      - '.'
  
  # Push image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/$PROJECT_ID/my-app:$SHORT_SHA'
  
  # Deploy to Cloud Run
  - name: 'gcr.io/cloud-builders/gke-deploy'
    args:
      - 'run'
      - '--filename=k8s/'
      - '--image=gcr.io/$PROJECT_ID/my-app:$SHORT_SHA'
      - '--location=us-central1'
      - '--cluster=my-cluster'

images:
  - 'gcr.io/$PROJECT_ID/my-app:$SHORT_SHA'
```

Triggers:
├─ Push to branch
├─ Pull request
├─ Tag push
├─ Manual trigger
└─ Scheduled (cron)

Substitutions:
├─ $PROJECT_ID: Your GCP project ID
├─ $SHORT_SHA: First 7 chars of commit hash
├─ $BRANCH_NAME: Git branch name
├─ $COMMIT_SHA: Full commit hash
└─ Custom variables: Define your own

Cost:
├─ First 120 minutes free per day
├─ $0.003/minute after that
└─ Machine types available (more expensive for higher specs)
```

### Artifact Registry
```
Central repository for artifacts (images, packages, libraries)

Artifact Types:
├─ Docker images
├─ Maven packages (Java)
├─ npm packages (Node.js)
├─ Python packages (pip)
├─ APT/YUM packages
└─ Generic artifacts

Setup:
```
# Create repository
gcloud artifacts repositories create my-repo \
    --repository-format=docker \
    --location=us-central1

# Configure Docker authentication
gcloud auth configure-docker us-central1-docker.pkg.dev

# Tag and push image
docker tag my-app:latest \
    us-central1-docker.pkg.dev/my-project/my-repo/my-app:latest

docker push \
    us-central1-docker.pkg.dev/my-project/my-repo/my-app:latest
```

Benefits:
├─ Private registry (no public Docker Hub)
├─ Integration with IAM (who can pull/push)
├─ Faster pulls (lower latency)
├─ Vulnerability scanning
└─ Image signing and verification
```

### Deployment Manager
```
Infrastructure as Code (template-based)

Templates:
├─ YAML or Jinja2
├─ Define GCP resources
├─ Reusable templates
└─ Version control friendly

Example Template (YAML):
```yaml
resources:
  - name: my-instance
    type: compute.v1.instances
    properties:
      zone: us-central1-a
      machineType: zones/us-central1-a/machineTypes/n1-standard-1
      disks:
        - boot: true
          initializeParams:
            sourceImage: projects/debian-cloud/global/images/debian-10
      networkInterfaces:
        - network: global/networks/default
          accessConfigs:
            - name: external-nat
```

Deployment:
```
$ gcloud deployment-manager deployments create my-deployment \
    --config=template.yaml

$ gcloud deployment-manager deployments update my-deployment \
    --config=new-template.yaml

$ gcloud deployment-manager deployments delete my-deployment
```

Alternative: Terraform (more popular than Deployment Manager)
```

### Terraform on GCP
```
Infrastructure as Code (declarative, popular)

Benefits:
├─ Multi-cloud (GCP, AWS, Azure)
├─ Modular and reusable
├─ State management
├─ Plan before apply (safe)
└─ Extensive GCP provider support

Basic Structure:
```hcl
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
}

provider "google" {
  project = var.project_id
  region  = var.region
}

resource "google_compute_instance" "my_instance" {
  name         = "my-instance"
  machine_type = "n1-standard-1"
  zone         = "${var.region}-a"

  boot_disk {
    initialize_params {
      image = "debian-cloud/debian-10"
    }
  }

  network_interface {
    network = "default"
    access_config {}
  }
}

output "instance_ip" {
  value = google_compute_instance.my_instance.network_interface[0].access_config[0].nat_ip
}
```

Workflow:
```
$ terraform init        # Initialize (download providers)
$ terraform plan       # See what will change
$ terraform apply      # Apply changes
$ terraform destroy    # Delete resources
```

State Management:
├─ terraform.tfstate: Current infrastructure state
├─ Store in GCS: Shared team state
├─ Remote backends: Safe collaboration
└─ State locking: Prevent concurrent modifications
```

---

## Monitoring & Logging

### Cloud Monitoring (Stackdriver)
```
Metrics collection, storage, and visualization

Metrics:
├─ Resource metrics (CPU, memory, disk)
├─ Application metrics (custom)
├─ Uptime checks (synthetic monitoring)
└─ Can track any metric you send

Collection:
├─ Monitoring agent (Linux, Windows)
├─ Google Cloud SDKs
├─ Prometheus (integrated)
└─ Custom applications (OpenTelemetry)

Dashboards:
```
Create custom dashboards showing:
├─ Key metrics for your service
├─ Status of dependencies
├─ Alerts at a glance
└─ Drill-down capability
```

Alerting:
```
Alert Policy: Condition → Notification

Conditions:
├─ Metric threshold (CPU > 80%)
├─ Absence of data
├─ Percent change
└─ Forecast (trending towards breach)

Notification:
├─ Email
├─ Slack
├─ PagerDuty
├─ Webhooks
└─ SMS
```

Uptime Checks:
```
Synthetic monitoring (probe your endpoint)

├─ HTTP/HTTPS checks (status + latency)
├─ TCP checks
└─ Run from multiple regions

Useful for:
├─ Alerting before users notice
├─ Detecting issues outside your region
├─ SLA verification
```
```

### Cloud Logging
```
Centralized logging for all GCP resources

Log Collection:
├─ Application logs (stdout/stderr in Cloud Run/GKE)
├─ Cloud Functions logs
├─ Cloud Build logs
├─ Audit logs (who did what)
├─ VPC Flow logs (network traffic)
└─ Custom logs (via Logging API)

Log Retention:
├─ Default: 30 days
├─ Configurable: 1 day to indefinite
├─ Export to Cloud Storage (for long-term)
└─ Cost: Per GB ingested (first 50GB free)

Viewing Logs:
```
# gcloud command
$ gcloud logging read "resource.type=cloud_run_revision"

# BigQuery
SELECT jsonPayload.message, severity, timestamp
FROM myproject.dataset.logs
WHERE resource.type = "cloud_function"
  AND timestamp > TIMESTAMP_SUB(NOW(), INTERVAL 1 HOUR);
```

Log Routing:
```
Sink: Route logs to different destinations

Example:
├─ ERROR logs → Email alert (fast response)
├─ INFO logs → Cloud Storage (cheap storage)
├─ All logs → BigQuery (analysis)
└─ Security logs → separate project (isolated)
```

Structured Logging:
```
Instead of: "User 123 logged in from 192.168.1.1"
Use: 
{
  "severity": "INFO",
  "message": "User login",
  "userId": 123,
  "ipAddress": "192.168.1.1",
  "timestamp": "2024-12-24T10:00:00Z"
}

Benefits:
├─ Queryable fields
├─ Easy filtering
├─ Better analytics
```
```

### Cloud Trace
```
Distributed tracing (see request flow through services)

How it Works:
```
1. Request arrives
2. System assigns Trace ID
3. Each service adds Span (timing for that service)
4. Spans collected and stored
5. View full trace: Which service is slow?
```

Example Trace:
```
GET /api/users/123 (total: 850ms)
├─ User Service (100ms)
│  └─ Parse request: 10ms
│  └─ Query user: 80ms
│  └─ Serialize JSON: 10ms
├─ Auth Service (50ms)
│  └─ Validate token: 50ms
├─ Recommendation Service (700ms) ← BOTTLENECK!
│  └─ Query database: 650ms
│  └─ Filter results: 50ms
└─ Return response: 10ms
```

Integration:
├─ OpenTelemetry (standards-based)
├─ Manual instrumentation
├─ Auto-instrumentation (some runtimes)
└─ Spans exported to Cloud Trace

Benefits:
├─ Find bottlenecks
├─ Understand service dependencies
├─ Improve performance
└─ Debug distributed issues
```

### Cloud Profiler
```
Continuous profiling (CPU, memory usage by code)

Detects:
├─ CPU hotspots (which functions use most CPU)
├─ Memory allocations (which code allocates memory)
├─ Lock contention
└─ Low-level issues

Example Profile:
```
Function calls and CPU time:
├─ query_database(): 45% CPU time (slow query!)
├─ serialize_response(): 30% CPU time
├─ auth_check(): 20% CPU time
└─ parse_request(): 5% CPU time

Action: Optimize database query → Performance +40%
```

Supported:
├─ Python, Java, Node.js, Go
├─ Minimal overhead
└─ Continuous (always running)

When to Use:
├─ Production performance bottlenecks
├─ Memory leak detection
└─ Optimization opportunities
```

---

## Best Practices

### Architecture Patterns

#### 1. Microservices on GKE
```
Components:
├─ GKE cluster (container orchestration)
├─ Multiple services (deployed as pods)
├─ Cloud SQL (shared database)
├─ Pub/Sub (async communication)
├─ Cloud Storage (file storage)
└─ Cloud Monitoring (observability)

Benefits:
├─ Independent scaling per service
├─ Independent deployment
├─ Technology diversity (polyglot)
├─ Failure isolation

Challenges:
├─ Operational complexity
├─ Network latency between services
├─ Data consistency (eventual consistency)
├─ Debugging across services (need tracing)

Solution: Use distributed tracing, logging, monitoring
```

#### 2. Serverless Web Application
```
Components:
├─ Cloud Storage (static assets)
├─ Cloud Run (API backend)
├─ Firestore (database)
├─ Cloud Tasks (async jobs)
├─ Cloud Scheduler (cron jobs)
└─ Cloud Monitoring (observability)

Benefits:
├─ No infrastructure management
├─ Automatic scaling
├─ Pay only for usage
├─ Fast deployment (git push)

Use Cases:
├─ Startups / MVPs (low initial cost)
├─ Variable traffic (good scaling)
├─ Event-driven workloads
└─ Prototype applications

Limitations:
├─ Execution time limits (Cloud Run: 60m)
├─ Cold starts (noticeable latency)
├─ Limited customization
```

#### 3. Multi-Region Disaster Recovery
```
Setup:
├─ Primary region: us-central1
├─ Secondary region: europe-west1
├─ Cloud SQL cross-region replica
├─ Cloud Storage multi-region bucket
├─ Global HTTPS Load Balancer
└─ Cloud Armor DDoS protection

Failover:
```
Primary region DOWN
↓
Load balancer detects (health checks)
↓
Routes traffic to secondary
↓
Database replica promoted to primary
↓
Service restored (usually < 1 minute)
```

RTO (Recovery Time Objective): < 1 minute
RPO (Recovery Point Objective): < 1 minute
```

### Security Best Practices

```
1. Identity & Access Management
   ├─ Use service accounts (not personal accounts)
   ├─ Principle of least privilege
   ├─ Regular access reviews
   ├─ Enable audit logging
   └─ Use Workload Identity (Kubernetes)

2. Network Security
   ├─ Use VPC (not public internet)
   ├─ Cloud NAT for private VM internet access
   ├─ Firewall rules (deny by default)
   ├─ Cloud Armor for DDoS/WAF
   └─ VPC Service Controls for data isolation

3. Data Security
   ├─ Encryption at rest (Cloud KMS)
   ├─ Encryption in transit (TLS, VPN)
   ├─ Data classification (secrets, PII)
   ├─ Secrets in Secret Manager (not code)
   └─ Regular backups

4. Application Security
   ├─ Container image scanning (Artifact Registry)
   ├─ Binary Authorization (only verified images)
   ├─ Regular patching
   ├─ Security testing (SAST, DAST)
   └─ Dependency scanning

5. Compliance & Audit
   ├─ Audit logging enabled
   ├─ Regular security audits
   ├─ Compliance checks (CIS benchmarks)
   ├─ Data retention policies
   └─ Disaster recovery testing
```

### Cost Optimization

```
1. Compute Optimization
   ├─ Use preemptible VMs (80% cheaper)
   ├─ Committed use discounts (3-year: 70% off)
   ├─ Right-size instances (monitor utilization)
   ├─ Auto-scaling (avoid overprovisioning)
   └─ Instance scheduling (stop dev/test instances at night)

2. Storage Optimization
   ├─ Use appropriate storage class
   │  ├─ Hot data: Standard
   │  ├─ Warm data: Nearline
   │  ├─ Cold data: Coldline / Archive
   ├─ Lifecycle policies (transition classes)
   ├─ Delete old data
   └─ Compression (reduce size)

3. Network Optimization
   ├─ Cloud CDN (reduce origin load)
   ├─ Avoid egress charges (keep data in region)
   ├─ Cloud Interconnect vs VPN (large transfers)
   └─ Caching (reduce API calls)

4. Monitoring & Logging Cost
   ├─ Log sampling (don't log everything)
   ├─ Log tiering (move old logs to GCS)
   ├─ Metric aggregation (don't send raw metrics)
   ├─ Dashboard consolidation
   └─ Use free tier (first 50GB logs, 150MB profiles)

5. Database Optimization
   ├─ Right-size Cloud SQL instances
   ├─ Use read replicas (not additional primaries)
   ├─ Firestore vs Cloud SQL (different pricing models)
   ├─ BigQuery slot-based pricing (for heavy use)
   └─ Regular backups (not excessive)
```

### Performance Best Practices

```
1. Application Performance
   ├─ Caching (memory, HTTP caching)
   ├─ Database optimization (indexes, query efficiency)
   ├─ Connection pooling
   ├─ Async processing (offload long tasks)
   └─ CDN for static assets

2. Scaling Patterns
   ├─ Vertical: Larger instance (simple, limited)
   ├─ Horizontal: More instances (better, needs load balancer)
   ├─ Auto-scaling: Based on metrics (ideal)
   └─ Database: Replication, sharding, federation

3. Latency Reduction
   ├─ Multi-region deployment
   ├─ Edge locations (Cloud CDN, Cloud Armor)
   ├─ Optimize cold starts (Cloud Run)
   └─ Minimize network hops

4. Debugging Performance Issues
   ├─ Cloud Trace: Service-to-service latency
   ├─ Cloud Profiler: CPU/memory hotspots
   ├─ Query Insights: Database slowness
   ├─ Network Monitoring: Bandwidth/latency
   └─ Logs + Metrics: Correlate issues
```

---

## Common GCP Commands

```bash
# Authentication
gcloud auth login                    # Login to GCP
gcloud config set project PROJECT_ID # Set default project

# Compute Engine
gcloud compute instances list                       # List VMs
gcloud compute instances create INSTANCE --zone=ZONE  # Create VM
gcloud compute instances ssh INSTANCE --zone=ZONE  # SSH into VM
gcloud compute instances delete INSTANCE --zone=ZONE  # Delete VM

# Cloud Run
gcloud run deploy SERVICE --source .                # Deploy from source
gcloud run services list                           # List services
gcloud run services delete SERVICE                 # Delete service
gcloud run logs read SERVICE                       # View logs

# Cloud SQL
gcloud sql instances list                          # List databases
gcloud sql connect INSTANCE                        # Connect to database
gcloud sql backups list --instance=INSTANCE        # List backups

# Cloud Storage
gsutil ls                                          # List buckets
gsutil mb gs://bucket-name                         # Create bucket
gsutil cp local-file.txt gs://bucket-name          # Upload file
gsutil cp gs://bucket-name/file.txt local.txt      # Download file

# GKE
gcloud container clusters list                     # List clusters
gcloud container clusters create CLUSTER           # Create cluster
gcloud container clusters get-credentials CLUSTER  # Get kubeconfig
kubectl get pods                                   # List pods (after kubeconfig setup)

# IAM
gcloud projects get-iam-policy PROJECT_ID          # List IAM bindings
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member=user:email@example.com \
  --role=roles/editor                             # Grant role

# Logging
gcloud logging read "resource.type=cloud_function" # View logs
gcloud logging write my-app "Hello GCP" --severity INFO  # Write to logs

# Monitoring
gcloud monitoring timeseries list                  # List metrics
```

---

## Practical Applications & Real-World Examples

### Building a Scalable E-Commerce Platform

**Architecture Overview:**
```
┌─────────────────────────────────────────────────────────┐
│         Cloud CDN + Cloud Armor (DDoS Protection)        │
└──────────────────┬──────────────────────────────────────┘
                   ↓
┌─────────────────────────────────────────────────────────┐
│          Global HTTPS Load Balancer                      │
│  (routes traffic based on URL path and hostname)         │
└──────────────────┬──────────────────────────────────────┘
                   ↓
         ┌─────────────────────┐
         │   Microservices     │
         ├─────────────────────┤
         │ • Product Service   │ (GKE pods)
         │ • Order Service     │
         │ • Payment Service   │
         │ • User Service      │
         └──────────┬──────────┘
                    ↓
    ┌───────────────────────────────────┐
    │  Cloud SQL (MySQL)                │
    │  ├─ Primary: us-central1          │
    │  ├─ Replica: europe-west1        │
    │  └─ Read replica: asia-east1     │
    └───────────────────────────────────┘
                    ↓
    ┌───────────────────────────────────┐
    │  Memorystore (Redis)              │
    │  - Session caching                │
    │  - Product cache                  │
    │  - Rate limiting                  │
    └───────────────────────────────────┘
                    ↓
    ┌───────────────────────────────────┐
    │  Cloud Storage                    │
    │  - Product images                 │
    │  - User uploads                   │
    │  - Invoices (archive tier)        │
    └───────────────────────────────────┘
```

**Step 1: Set up GKE Cluster**
```bash
# Create GKE cluster (Autopilot recommended)
gcloud container clusters create e-commerce-cluster \
    --enable-autopilot \
    --region us-central1 \
    --project my-project \
    --enable-stackdriver-kubernetes \
    --enable-ip-alias \
    --network-policy \
    --addons HttpLoadBalancing,HorizontalPodAutoscaling \
    --enable-shielded-nodes

# Get credentials
gcloud container clusters get-credentials e-commerce-cluster \
    --region us-central1 \
    --project my-project

# Verify connection
kubectl cluster-info
kubectl get nodes
```

**Step 2: Deploy Services**
```yaml
# product-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-service
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: product-service
  template:
    metadata:
      labels:
        app: product-service
    spec:
      serviceAccountName: product-service
      containers:
      - name: product-service
        image: us-central1-docker.pkg.dev/my-project/my-repo/product-service:v1.0
        ports:
        - containerPort: 8080
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: cloudsql-credentials
              key: DATABASE_URL
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: redis-config
              key: REDIS_URL
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: product-service
  labels:
    app: product-service
spec:
  type: ClusterIP
  selector:
    app: product-service
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: product-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: product-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Step 3: Configure Cloud SQL**
```bash
# Create Cloud SQL instance
gcloud sql instances create ecommerce-db \
    --database-version=MYSQL_8_0 \
    --tier=db-n1-standard-2 \
    --region=us-central1 \
    --backup-start-time 03:00 \
    --enable-bin-log \
    --retained-backups-count=30 \
    --transaction-log-retention-days=7 \
    --database-flags=cloudsql_iam_authentication=on

# Create databases
gcloud sql databases create ecommerce \
    --instance=ecommerce-db
gcloud sql databases create analytics \
    --instance=ecommerce-db

# Create service account for application
gcloud iam service-accounts create cloudsql-app \
    --display-name "Cloud SQL Application"

# Grant permissions
gcloud projects add-iam-policy-binding my-project \
    --member=serviceAccount:cloudsql-app@my-project.iam.gserviceaccount.com \
    --role=roles/cloudsql.client

# Create user
gcloud sql users create app_user \
    --instance=ecommerce-db \
    --type=CLOUD_IAM_SERVICE_ACCOUNT
```

**Step 4: Application Code Example**
```python
# app.py - Flask API for product service
from flask import Flask, jsonify, request
from flask_cors import CORS
import os
from datetime import datetime, timedelta
import redis
import logging
from google.cloud import sql_connector
from functools import wraps
import json

app = Flask(__name__)
CORS(app)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize connectors
sql_connector_instance = sql_connector.Connector()

# Redis for caching
redis_client = redis.Redis.from_url(os.getenv('REDIS_URL'))

# Database connection
def get_db_connection():
    return sql_connector_instance.connect(
        "my-project:us-central1:ecommerce-db",
        "pymysql",
        user="app_user@my-project.iam",
        db="ecommerce",
        enable_iam_auth=True,
    )

# Caching decorator
def cached(expiration=300):
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            cache_key = f"{f.__name__}:{args}:{kwargs}"
            
            # Try to get from cache
            cached_result = redis_client.get(cache_key)
            if cached_result:
                logger.info(f"Cache HIT for {cache_key}")
                return json.loads(cached_result)
            
            # Cache miss, execute function
            result = f(*args, **kwargs)
            
            # Store in cache
            redis_client.setex(
                cache_key,
                expiration,
                json.dumps(result)
            )
            logger.info(f"Cache MISS for {cache_key}, stored for {expiration}s")
            return result
        return decorated_function
    return decorator

# Health check endpoints
@app.route('/health', methods=['GET'])
def health():
    """Liveness probe - is service running?"""
    return jsonify({"status": "healthy"}), 200

@app.route('/ready', methods=['GET'])
def ready():
    """Readiness probe - can service handle traffic?"""
    try:
        # Check database connection
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT 1")
        cursor.close()
        conn.close()
        
        # Check Redis connection
        redis_client.ping()
        
        return jsonify({"status": "ready"}), 200
    except Exception as e:
        logger.error(f"Readiness check failed: {e}")
        return jsonify({"status": "not_ready", "error": str(e)}), 503

# Product endpoints
@app.route('/api/products', methods=['GET'])
@cached(expiration=600)  # Cache for 10 minutes
def get_products():
    """Get all products with pagination"""
    page = request.args.get('page', 1, type=int)
    limit = request.args.get('limit', 20, type=int)
    offset = (page - 1) * limit
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor(dictionary=True)
        
        # Get total count
        cursor.execute("SELECT COUNT(*) as count FROM products")
        total = cursor.fetchone()['count']
        
        # Get paginated results
        cursor.execute("""
            SELECT id, name, description, price, stock, created_at
            FROM products
            ORDER BY created_at DESC
            LIMIT %s OFFSET %s
        """, (limit, offset))
        
        products = cursor.fetchall()
        cursor.close()
        conn.close()
        
        logger.info(f"Retrieved {len(products)} products (page {page})")
        
        return jsonify({
            "data": products,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "pages": (total + limit - 1) // limit
            }
        }), 200
        
    except Exception as e:
        logger.error(f"Error fetching products: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/products/<int:product_id>', methods=['GET'])
@cached(expiration=300)
def get_product(product_id):
    """Get single product"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor(dictionary=True)
        cursor.execute("""
            SELECT id, name, description, price, stock, created_at
            FROM products
            WHERE id = %s
        """, (product_id,))
        
        product = cursor.fetchone()
        cursor.close()
        conn.close()
        
        if not product:
            return jsonify({"error": "Product not found"}), 404
        
        logger.info(f"Retrieved product {product_id}")
        return jsonify(product), 200
        
    except Exception as e:
        logger.error(f"Error fetching product {product_id}: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/products', methods=['POST'])
def create_product():
    """Create new product"""
    data = request.get_json()
    
    # Validate input
    required_fields = ['name', 'price', 'stock']
    if not all(field in data for field in required_fields):
        return jsonify({"error": "Missing required fields"}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO products (name, description, price, stock, created_at)
            VALUES (%s, %s, %s, %s, %s)
        """, (
            data['name'],
            data.get('description', ''),
            data['price'],
            data['stock'],
            datetime.utcnow()
        ))
        
        conn.commit()
        product_id = cursor.lastrowid
        cursor.close()
        conn.close()
        
        # Invalidate cache
        redis_client.delete("get_products:*")
        
        logger.info(f"Created product {product_id}")
        return jsonify({"id": product_id, "message": "Product created"}), 201
        
    except Exception as e:
        logger.error(f"Error creating product: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/products/<int:product_id>/stock', methods=['PATCH'])
def update_stock(product_id):
    """Update product stock"""
    data = request.get_json()
    new_stock = data.get('stock')
    
    if new_stock is None:
        return jsonify({"error": "Stock value required"}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            UPDATE products
            SET stock = %s, updated_at = %s
            WHERE id = %s
        """, (new_stock, datetime.utcnow(), product_id))
        
        if cursor.rowcount == 0:
            return jsonify({"error": "Product not found"}), 404
        
        conn.commit()
        cursor.close()
        conn.close()
        
        # Invalidate cache
        redis_client.delete(f"get_product:{product_id}")
        
        logger.info(f"Updated stock for product {product_id} to {new_stock}")
        return jsonify({"message": "Stock updated"}), 200
        
    except Exception as e:
        logger.error(f"Error updating stock: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=False)
```

**Step 5: Deploy with Cloud Build**
```yaml
# cloudbuild.yaml
steps:
  # Test
  - name: 'python:3.11'
    entrypoint: pip
    args: ['install', '-r', 'requirements.txt']
  
  - name: 'python:3.11'
    entrypoint: pytest
    args: ['tests/']
  
  # Build image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'us-central1-docker.pkg.dev/$PROJECT_ID/my-repo/product-service:$SHORT_SHA'
      - '-t'
      - 'us-central1-docker.pkg.dev/$PROJECT_ID/my-repo/product-service:latest'
      - '.'
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=us-central1-a'
      - 'CLOUDSDK_CONTAINER_CLUSTER=e-commerce-cluster'
  
  # Push to Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'us-central1-docker.pkg.dev/$PROJECT_ID/my-repo/product-service:$SHORT_SHA'
  
  # Deploy to GKE
  - name: 'gcr.io/cloud-builders/kubectl'
    args:
      - 'set'
      - 'image'
      - 'deployment/product-service'
      - 'product-service=us-central1-docker.pkg.dev/$PROJECT_ID/my-repo/product-service:$SHORT_SHA'
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=us-central1-a'
      - 'CLOUDSDK_CONTAINER_CLUSTER=e-commerce-cluster'
  
  # Wait for rollout
  - name: 'gcr.io/cloud-builders/kubectl'
    args:
      - 'rollout'
      - 'status'
      - 'deployment/product-service'
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=us-central1-a'
      - 'CLOUDSDK_CONTAINER_CLUSTER=e-commerce-cluster'

images:
  - 'us-central1-docker.pkg.dev/$PROJECT_ID/my-repo/product-service:$SHORT_SHA'
  - 'us-central1-docker.pkg.dev/$PROJECT_ID/my-repo/product-service:latest'

substitutions:
  _CLUSTER_NAME: "e-commerce-cluster"
  _CLUSTER_ZONE: "us-central1-a"

timeout: '1200s'
```

**Step 6: Monitoring Setup**
```yaml
# alerts.yaml - Prometheus-style alerts
groups:
  - name: product-service
    rules:
      - alert: ProductServiceDown
        expr: up{job="product-service"} == 0
        for: 1m
        annotations:
          summary: "Product Service is down"
          description: "Product service has been down for > 1 minute"
      
      - alert: HighErrorRate
        expr: |
          (sum(rate(http_requests_total{status=~"5.."}[5m])) /
           sum(rate(http_requests_total[5m]))) > 0.05
        for: 5m
        annotations:
          summary: "High error rate (>5%)"
          description: "Error rate is {{ $value | humanizePercentage }}"
      
      - alert: HighLatency
        expr: |
          histogram_quantile(0.99, 
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1
        for: 5m
        annotations:
          summary: "High P99 latency (>1s)"
          description: "P99 latency is {{ $value | humanizeDuration }}"
      
      - alert: HighDatabaseConnections
        expr: mysql_global_status_threads_connected > 80
        for: 5m
        annotations:
          summary: "High database connections (>80)"
          description: "Current connections: {{ $value }}"
```

---

### Building a Real-Time Analytics Dashboard

**Use Case:** Track user behavior in real-time

**Architecture:**
```
┌─────────────────────────────────────────────┐
│          Web/Mobile Application              │
│  (sends events: page views, clicks, etc.)    │
└────────────────────┬────────────────────────┘
                     ↓
       ┌─────────────────────────────┐
       │   Cloud Pub/Sub             │
       │  (event queue, 1M msg/sec)  │
       └────────────┬────────────────┘
         ┌──────────┴──────────┐
         ↓                     ↓
   ┌──────────────┐    ┌──────────────────┐
   │ Cloud Run    │    │ Dataflow Pipeline│
   │ (real-time)  │    │ (batch + stream) │
   └──────────────┘    └────────┬─────────┘
         ↓                       ↓
   ┌──────────────┐    ┌──────────────────┐
   │ Firestore    │    │ BigQuery         │
   │ (live stats) │    │ (analytics)      │
   └──────────────┘    └────────┬─────────┘
         ↓                       ↓
   ┌──────────────┐    ┌──────────────────┐
   │ Cloud Run    │    │ Looker           │
   │ Dashboard    │    │ (dashboards)     │
   └──────────────┘    └──────────────────┘
```

**Event Schema:**
```python
# events.py
from dataclasses import dataclass, asdict
from datetime import datetime
import json
from typing import Optional

@dataclass
class UserEvent:
    """Schema for user events"""
    event_id: str
    user_id: str
    event_type: str  # page_view, click, scroll, form_submit
    session_id: str
    timestamp: datetime
    url: str
    user_agent: str
    ip_address: str
    event_data: dict  # Event-specific data
    
    def to_json(self) -> str:
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        return json.dumps(data)

# Example events
page_view = UserEvent(
    event_id="evt_123456",
    user_id="user_789",
    event_type="page_view",
    session_id="sess_abc",
    timestamp=datetime.utcnow(),
    url="https://example.com/products",
    user_agent="Mozilla/5.0...",
    ip_address="192.168.1.1",
    event_data={
        "page_title": "Products",
        "referrer": "https://example.com",
        "page_load_time_ms": 1250
    }
)

form_submit = UserEvent(
    event_id="evt_123457",
    user_id="user_789",
    event_type="form_submit",
    session_id="sess_abc",
    timestamp=datetime.utcnow(),
    url="https://example.com/checkout",
    user_agent="Mozilla/5.0...",
    ip_address="192.168.1.1",
    event_data={
        "form_name": "checkout",
        "form_fields": ["email", "address", "payment"],
        "submission_time_ms": 5000
    }
)
```

**Pub/Sub Publisher:**
```python
# publisher.py
from google.cloud import pubsub_v1
import json
from events import UserEvent

class EventPublisher:
    def __init__(self, project_id: str, topic_id: str):
        self.publisher = pubsub_v1.PublisherClient()
        self.topic_path = self.publisher.topic_path(project_id, topic_id)
    
    def publish(self, event: UserEvent) -> str:
        """Publish event to Pub/Sub"""
        # Serialize event
        event_json = event.to_json()
        
        # Publish with retries
        future = self.publisher.publish(
            self.topic_path,
            event_json.encode('utf-8'),
            event_type=event.event_type,
            user_id=event.user_id,
            timestamp=event.timestamp.isoformat()
        )
        
        # Get message ID
        message_id = future.result(timeout=5.0)
        return message_id
    
    def publish_batch(self, events: list[UserEvent]) -> list[str]:
        """Publish multiple events efficiently"""
        message_ids = []
        for event in events:
            message_id = self.publish(event)
            message_ids.append(message_id)
        
        # Wait for all to complete
        self.publisher.api.flush()
        return message_ids

# Usage
publisher = EventPublisher('my-project', 'user-events')
message_id = publisher.publish(page_view)
print(f"Published event: {message_id}")
```

**Dataflow Pipeline (Stream Processing):**
```python
# dataflow_pipeline.py
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.transforms.window import FixedWindows, SlidingWindows
import json
from datetime import datetime

class ParseEvent(beam.DoFn):
    """Parse JSON event"""
    def process(self, element):
        try:
            event = json.loads(element)
            yield event
        except json.JSONDecodeError as e:
            # Log error, skip bad events
            print(f"Error parsing event: {e}")

class EnrichEvent(beam.DoFn):
    """Add derived fields"""
    def process(self, event):
        # Add processing timestamp
        event['processed_at'] = datetime.utcnow().isoformat()
        
        # Extract date for partitioning
        timestamp = datetime.fromisoformat(event['timestamp'])
        event['date'] = timestamp.strftime('%Y-%m-%d')
        
        yield event

class AggregateMetrics(beam.DoFn):
    """Aggregate metrics in windows"""
    def process(self, element, window=beam.transforms.window.GlobalWindow()):
        # element is list of events in the window
        event_types = {}
        total_events = len(element)
        
        for event in element:
            event_type = event['event_type']
            event_types[event_type] = event_types.get(event_type, 0) + 1
        
        yield {
            'timestamp': datetime.utcnow().isoformat(),
            'total_events': total_events,
            'event_breakdown': event_types,
            'window_end': window.end.isoformat()
        }

class BigQueryTableRow(beam.DoFn):
    """Format for BigQuery"""
    def process(self, event):
        row = {
            'event_id': event['event_id'],
            'user_id': event['user_id'],
            'event_type': event['event_type'],
            'timestamp': event['timestamp'],
            'url': event['url'],
            'processed_at': event['processed_at'],
            'date': event['date'],
            'event_data': json.dumps(event['event_data'])
        }
        yield row

def run_pipeline(project_id, input_topic, output_table):
    """Run streaming pipeline"""
    options = PipelineOptions(
        project=project_id,
        runner='DataflowRunner',
        region='us-central1',
        temp_location='gs://my-temp-bucket/temp',
        staging_location='gs://my-temp-bucket/staging',
        job_name='event-analytics-pipeline',
        autoscaling_algorithm='THROUGHPUT_BASED',
        num_workers=2,
        max_num_workers=10
    )
    
    with beam.Pipeline(options=options) as pipeline:
        # Read from Pub/Sub
        raw_events = (
            pipeline
            | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(
                topic=f'projects/{project_id}/topics/user-events'
            )
        )
        
        # Parse events
        parsed_events = (
            raw_events
            | 'Parse Events' >> beam.ParDo(ParseEvent())
        )
        
        # Enrich events
        enriched_events = (
            parsed_events
            | 'Enrich Events' >> beam.ParDo(EnrichEvent())
        )
        
        # Write to BigQuery (streaming inserts)
        (
            enriched_events
            | 'Format for BQ' >> beam.ParDo(BigQueryTableRow())
            | 'Write to BigQuery' >> beam.io.WriteToBigQuery(
                table=output_table,
                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,
                schema={
                    'fields': [
                        {'name': 'event_id', 'type': 'STRING'},
                        {'name': 'user_id', 'type': 'STRING'},
                        {'name': 'event_type', 'type': 'STRING'},
                        {'name': 'timestamp', 'type': 'TIMESTAMP'},
                        {'name': 'url', 'type': 'STRING'},
                        {'name': 'processed_at', 'type': 'TIMESTAMP'},
                        {'name': 'date', 'type': 'DATE'},
                        {'name': 'event_data', 'type': 'JSON'}
                    ]
                }
            )
        )
        
        # Also aggregate for real-time metrics
        aggregated = (
            enriched_events
            | 'Window Events (5 min)' >> beam.WindowInto(
                FixedWindows(300)  # 5-minute windows
            )
            | 'Group for Aggregation' >> beam.Map(lambda x: (x['event_type'], 1))
            | 'Aggregate' >> beam.CombinePerKey(sum)
            | 'Format Metrics' >> beam.Map(
                lambda x: {
                    'event_type': x[0],
                    'count': x[1],
                    'timestamp': datetime.utcnow().isoformat()
                }
            )
            | 'Write Metrics to BigQuery' >> beam.io.WriteToBigQuery(
                table=f'{output_table.replace("events", "metrics")}',
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
            )
        )

if __name__ == '__main__':
    run_pipeline(
        project_id='my-project',
        input_topic='projects/my-project/topics/user-events',
        output_table='my-project:analytics.events'
    )
```

**Real-time Dashboard (Firestore + Cloud Run):**
```python
# dashboard_api.py - Flask API for live stats
from flask import Flask, jsonify
from google.cloud import firestore
from google.cloud import bigquery
from datetime import datetime, timedelta
import time

app = Flask(__name__)
db = firestore.Client()
bq_client = bigquery.Client()

@app.route('/api/stats/live', methods=['GET'])
def get_live_stats():
    """Get real-time stats (last 5 minutes)"""
    try:
        # Get from Firestore (updated every minute by Cloud Function)
        stats_doc = db.collection('metrics').document('live_stats').get()
        
        if not stats_doc.exists:
            return jsonify({"error": "No stats available"}), 404
        
        stats = stats_doc.to_dict()
        return jsonify(stats), 200
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/stats/daily', methods=['GET'])
def get_daily_stats():
    """Get stats for today from BigQuery"""
    try:
        today = datetime.utcnow().date()
        
        query = f"""
        SELECT
            event_type,
            COUNT(*) as count,
            COUNT(DISTINCT user_id) as unique_users,
            AVG(CAST(JSON_EXTRACT_SCALAR(event_data, '$.page_load_time_ms') AS INT64)) as avg_load_time
        FROM `my-project.analytics.events`
        WHERE date = '{today}'
        GROUP BY event_type
        ORDER BY count DESC
        """
        
        results = bq_client.query(query).result()
        
        stats = {
            'date': str(today),
            'events': []
        }
        
        for row in results:
            stats['events'].append({
                'event_type': row.event_type,
                'count': row.count,
                'unique_users': row.unique_users,
                'avg_load_time_ms': round(row.avg_load_time) if row.avg_load_time else None
            })
        
        return jsonify(stats), 200
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/stats/users/<user_id>', methods=['GET'])
def get_user_stats(user_id):
    """Get stats for specific user"""
    try:
        # Last 7 days of activity
        seven_days_ago = (datetime.utcnow() - timedelta(days=7)).date()
        
        query = f"""
        SELECT
            DATE(timestamp) as date,
            event_type,
            COUNT(*) as count
        FROM `my-project.analytics.events`
        WHERE user_id = '{user_id}'
            AND date >= '{seven_days_ago}'
        GROUP BY date, event_type
        ORDER BY date DESC, count DESC
        """
        
        results = bq_client.query(query).result()
        
        activity = {}
        for row in results:
            date_str = str(row.date)
            if date_str not in activity:
                activity[date_str] = []
            activity[date_str].append({
                'event_type': row.event_type,
                'count': row.count
            })
        
        return jsonify({
            'user_id': user_id,
            'activity': activity
        }), 200
    
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=False)
```

**Cloud Function to Update Live Stats:**
```python
# update_live_stats.py - Triggered every minute
from google.cloud import firestore
from google.cloud import bigquery
from datetime import datetime, timedelta
import functions_framework

db = firestore.Client()
bq_client = bigquery.Client()

@functions_framework.cloud_event
def update_live_stats(cloud_event):
    """Update live stats every minute"""
    
    # Last 5 minutes
    five_min_ago = (datetime.utcnow() - timedelta(minutes=5)).isoformat()
    
    query = f"""
    SELECT
        event_type,
        COUNT(*) as count,
        COUNT(DISTINCT user_id) as unique_users,
        COUNT(DISTINCT session_id) as sessions
    FROM `my-project.analytics.events`
    WHERE processed_at > '{five_min_ago}'
    GROUP BY event_type
    """
    
    results = bq_client.query(query).result()
    
    stats = {
        'updated_at': datetime.utcnow().isoformat(),
        'events': {}
    }
    
    total_count = 0
    for row in results:
        stats['events'][row.event_type] = {
            'count': row.count,
            'unique_users': row.unique_users,
            'sessions': row.sessions
        }
        total_count += row.count
    
    stats['total_events'] = total_count
    
    # Update Firestore
    db.collection('metrics').document('live_stats').set(stats)
    
    return 'Live stats updated', 200
```

---

### Machine Learning with Vertex AI

**Training a Custom Model:**
```python
# vertex_ai_training.py
from google.cloud import aiplatform
from google.cloud import storage
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import joblib

def train_model():
    """Train ML model and upload to Vertex AI"""
    
    # Initialize Vertex AI
    aiplatform.init(
        project='my-project',
        location='us-central1',
        staging_bucket='gs://my-training-bucket'
    )
    
    # Load training data from BigQuery
    query = """
    SELECT
        feature1, feature2, feature3, feature4,
        label
    FROM `my-project.ml_data.training_dataset`
    WHERE split = 'train'
    """
    
    df = pd.read_gbq(query, project_id='my-project')
    
    # Prepare data
    X = df.drop('label', axis=1)
    y = df['label']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Train model
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # Evaluate
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    
    print(f"Train accuracy: {train_score:.4f}")
    print(f"Test accuracy: {test_score:.4f}")
    
    # Save model
    model_path = '/tmp/model.pkl'
    joblib.dump(model, model_path)
    
    # Upload to Cloud Storage
    storage_client = storage.Client()
    bucket = storage_client.bucket('my-models-bucket')
    blob = bucket.blob('models/random-forest-v1/model.pkl')
    blob.upload_from_filename(model_path)
    
    print(f"Model uploaded to gs://my-models-bucket/models/random-forest-v1/model.pkl")

if __name__ == '__main__':
    train_model()
```

---

### Cost Optimization Real Example

**Scenario: Reducing $12,000/month to $3,000/month**

```
BEFORE:
├─ Compute Engine (3 x n1-standard-4): $600/month
├─ Cloud SQL (db-n1-standard-4): $3,000/month
├─ Cloud Storage (500GB standard): $10/month
├─ BigQuery (on-demand): $3,000/month
├─ Cloud Monitoring (50 dashboards): $1,000/month
├─ Dataflow jobs (continuous): $4,200/month
└─ Total: $12,000/month

AFTER (optimized):
├─ GKE Autopilot: $400/month (shared control plane)
├─ Cloud SQL (db-n1-standard-2, CUD): $700/month (-75%)
├─ Cloud Storage (tiered): $5/month (archive old data)
├─ BigQuery (slots-based CUD): $2,000/month (-33%)
├─ Cloud Monitoring (3 dashboards, sampling): $100/month (-90%)
├─ Dataflow (batch processing, scheduled): $500/month (-88%)
└─ Total: $3,705/month (69% reduction!)

Optimizations Applied:
1. ✓ Migrated from VMs to GKE (auto-scaling, efficiency)
2. ✓ Downgraded Cloud SQL instance (still handles load)
3. ✓ Committed use discounts (3-year contract)
4. ✓ Archive cold data (S3 Glacier equivalent)
5. ✓ Reduced monitoring metrics (keep only actionable)
6. ✓ Scheduled batch jobs instead of continuous
7. ✓ Reserved BigQuery slots (25% cheaper annually)
```

---

## Core Concepts Summary & Key Takeaways

### Architectural Patterns & Design Principles

**1. Scalability Patterns**

```
Vertical Scaling (Scale UP):
┌─────────────────────┐    ┌──────────────────────┐
│ n1-standard-1 (1CPU)│ → │ n1-standard-4 (4CPU) │
│ 3.75GB memory       │    │ 15GB memory          │
└─────────────────────┘    └──────────────────────┘

Characteristics:
├─ Faster path to larger capacity
├─ Limited by maximum machine size
├─ Requires downtime (single instance)
├─ Simpler initially (no load balancing)
└─ Doesn't solve reliability (single instance = SPOF)

When to use:
├─ Database servers (vertical scaling more common)
├─ Memory-intensive workloads
├─ When horizontal scaling not possible
└─ Temporary solution before horizontal scaling

Horizontal Scaling (Scale OUT):
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│ Instance 1   │  │ Instance 2   │  │ Instance 3   │
│ n1-std-1     │  │ n1-std-1     │  │ n1-std-1     │
└──────────┬───┘  └──────────┬───┘  └──────────┬───┘
           └──────────┬──────────────────┬──────┘
                      ↓
            ┌────────────────────┐
            │ Load Balancer      │
            └────────────────────┘

Characteristics:
├─ Add more instances instead of bigger instance
├─ Unlimited scalability (add as many as needed)
├─ No downtime (rolling deployment)
├─ Better reliability (one instance fails, others handle)
├─ Complex load balancing
└─ Requires stateless design (share state in DB)

When to use:
├─ Web applications
├─ APIs
├─ Microservices
├─ When reliability critical
└─ When growth unpredictable

Combination (Hybrid):
├─ Vertical: Medium-sized instances
├─ Horizontal: Multiple instances load-balanced
├─ Auto-scaling: Automatically adjust count
└─ Cost-optimal: Both efficiency and reliability
```

**2. Resilience Patterns**

```
Graceful Degradation:
Service A (Primary)  ─┬─→ Works? Return ✓
                     │
                     └─→ Fails? 
                         └─→ Service B (Fallback) ─→ Return degraded ✓

Timeout + Retry:
Request → Service ─┬─→ Response? Return ✓
                   │
                   ├─→ Timeout? Retry (exponential backoff)
                   │
                   └─→ Still fails? Return error or cached response

Circuit Breaker:
┌──────────────────────────────────┐
│ Circuit Breaker States           │
├──────────────────────────────────┤
│ CLOSED (normal)                  │
│ └─→ Try to call service          │
│     ├─→ Success: Stay CLOSED     │
│     └─→ Failure: Open breaker    │
│                                  │
│ OPEN (service broken)            │
│ └─→ Don't call service (fail fast)
│     ├─→ Time passes              │
│     └─→ Try HALF_OPEN            │
│                                  │
│ HALF_OPEN (testing recovery)     │
│ └─→ Try single request           │
│     ├─→ Success: Close breaker   │
│     └─→ Failure: Reopen          │
└──────────────────────────────────┘

Benefits:
├─ Fail fast (don't wait for timeout)
├─ Prevent cascading failures
├─ Give service time to recover
└─ Better user experience (quick error vs long wait)
```

**3. Data Consistency Patterns**

```
Consistency Levels (from strongest to weakest):

1. Strong Consistency
   Write → Replicate to all → Return ✓
   Read → Always fresh data
   Cost: High latency
   Use: Financial, medical, sensitive data

2. Causal Consistency
   Order of related events preserved
   But unrelated events can be in any order
   Cost: Medium latency
   Use: Social media (comments order matters)

3. Eventual Consistency
   Eventually all replicas match
   Temporarily stale reads possible
   Cost: Low latency
   Use: Analytics, caches, less critical data

4. Weak Consistency
   No guarantees on order or staleness
   Cost: Lowest latency
   Use: Metrics, logs, non-critical telemetry

Choosing level:
├─ More critical data = Stronger consistency = Higher latency
├─ Less critical data = Weaker consistency = Lower latency
├─ Eventual consistency for 99% of read-heavy workloads
└─ Strong consistency for write-heavy critical data
```

### Performance & Optimization Keypoints

```
Optimization Hierarchy (Do in order):

1. Architecture (Biggest impact)
   ├─ Right tool (BigQuery for analytics, not OLTP)
   ├─ Scaling strategy (horizontal > vertical)
   └─ CDN for static content (content delivery)

2. Application (Medium impact)
   ├─ Connection pooling (reuse DB connections)
   ├─ Query optimization (indexes, EXPLAIN ANALYZE)
   ├─ Caching (avoid repeated computation)
   └─ Async processing (don't wait for slow operations)

3. Infrastructure (Lower impact if architecture right)
   ├─ Right-size instances (monitor actual usage)
   ├─ Network optimization (colocation, reduced hops)
   └─ Disk optimization (SSD vs HDD, throughput)

Common Mistake:
└─ Try to optimize code before fixing architecture
   (Like optimizing SQL when you should use BigQuery)
```

### Cost Optimization Keypoints

```
Cost = Compute + Storage + Networking + Data Transfer

Optimization priority:

1. Right-size resources (40% of people over-provision)
   ├─ Monitor actual CPU/memory usage
   ├─ Right-size to 60-70% peak usage
   └─ Saves 30-40% immediate

2. Eliminate idle resources (30% are never used)
   ├─ Delete old disks, snapshots
   ├─ Stop dev/test instances at night
   ├─ Use scheduler to start/stop

3. Optimize storage class (20% of storage is cold)
   ├─ Move old data to Coldline/Archive
   ├─ Set lifecycle policies
   └─ Saves 80% on old data

4. Use commitments for predictable workloads (30-70% savings)
   ├─ CUD for stable production
   ├─ Preemptible for batch jobs
   └─ Combine for hybrid workload

5. Network optimization (10% of bill often overlooked)
   ├─ Keep data in same region (avoid egress)
   ├─ CDN for global distribution
   └─ Use Cloud Interconnect for large transfers

Typical savings breakdown:
├─ Right-sizing: 30-40% savings
├─ Storage optimization: 20% savings  
├─ Commitments: 30-70% savings (if applicable)
├─ Network: 10-20% savings
└─ Total potential: 60-80% cost reduction
```

### Security Keypoints

```
Security Layers (Defense in Depth):

1. Perimeter (Network layer)
   ├─ VPC isolation (private network)
   ├─ Firewall rules (allow/deny ports)
   └─ Cloud Armor (DDoS protection)

2. Identity (Authentication & Authorization)
   ├─ IAM (who can do what)
   ├─ Service accounts (app identities)
   └─ Audit logging (track all actions)

3. Data (Encryption & Privacy)
   ├─ Encryption at rest (Cloud KMS)
   ├─ Encryption in transit (TLS/mTLS)
   ├─ Data classification (secrets in Secret Manager)
   └─ PII handling (redaction, masking)

4. Application (Code security)
   ├─ Input validation (prevent injection)
   ├─ Least privilege (minimal permissions)
   ├─ Regular patching (dependencies)
   └─ Security testing (SAST, DAST)

5. Monitoring (Detection)
   ├─ Audit logs (who did what)
   ├─ Security alerts (anomalies)
   ├─ Intrusion detection (VPC Flow Logs)
   └─ Regular audits (compliance)

Don't do (Security Anti-patterns):
├─ Store secrets in code (breach risk)
├─ Use basic roles (Editor, Owner, Viewer)
├─ Public IP for databases (unnecessary exposure)
├─ Long-lived credentials (rotate regularly)
├─ Skip encryption (compliance requirement)
└─ Ignore audit logs (can't investigate incidents)
```

### Reliability Keypoints

```
Building Reliable Systems:

Target SLA (Service Level Agreement):
├─ 99% uptime = 3.7 hours downtime/month (acceptable for dev)
├─ 99.9% uptime = 43 minutes downtime/month (production)
├─ 99.95% uptime = 22 minutes downtime/month (critical)
├─ 99.99% uptime = 4 minutes downtime/month (highly critical)
└─ Cost doubles for each 9 (exponential)

Strategies to achieve SLA:

1. Multi-zone deployment
   ├─ If zone fails, traffic automatically routes to healthy zone
   ├─ Increases uptime from 99.5% to 99.95%
   └─ Minimal cost difference

2. Read replicas for databases
   ├─ Distributes read load
   ├─ Can be promoted if primary fails
   └─ Cheaper than active-active multi-master

3. Circuit breakers & fallbacks
   ├─ Don't cascade failures
   ├─ Return partial response instead of error
   └─ Improve perceived availability

4. Automated recovery
   ├─ Health checks (detect failures quickly)
   ├─ Automatic failover (switch traffic)
   ├─ Auto-scaling (handle load increases)
   └─ Reduces MTTR (mean time to recovery)

5. Monitoring & alerting
   ├─ Detect failures before users notice
   ├─ Alert on trending issues (before failure)
   └─ Reduce MTTD (mean time to detection)

Calculation Example:
├─ Single instance: 99.5% uptime
├─ Multi-zone: 1 - (0.5% * 0.5%) = 99.9975% uptime
├─ Different components: Multiply individual uptimes
│  (99.9% * 99.9% * 99% = 98.8% combined)
└─ Lesson: Weakest link determines overall reliability
```

```
When Asked About GCP:

1. Start with use case
   └─ "Are you building a web app, analytics platform, ML model?"

2. Match service to requirement
   └─ Web app + quick scale → Cloud Run
   └─ Complex queries on big data → BigQuery
   └─ Real-time data → Firestore
   └─ Existing k8s → GKE
   └─ Legacy apps → Compute Engine

3. Consider non-functional requirements
   ├─ Availability: Do you need multi-region?
   ├─ Latency: Do you need edge caching?
   ├─ Cost: Budget constraints?
   ├─ Compliance: Data residency requirements?
   └─ Scale: How many users/requests?

4. Discuss trade-offs
   └─ "Firestore is easy but BigQuery scales better for analytics"
   └─ "Cloud Run is simple but GKE is more flexible"
   └─ "Cloud SQL is simple but Spanner scales globally"

5. Security must-haves
   ├─ "I'd use IAM with service accounts"
   ├─ "Encryption at rest with KMS"
   ├─ "VPC to isolate network"
   ├─ "Audit logging for compliance"
   └─ "Secrets in Secret Manager"

6. Monitoring and observability
   ├─ "Cloud Monitoring for metrics"
   ├─ "Cloud Logging for structured logs"
   ├─ "Cloud Trace for distributed tracing"
   └─ "Alerts on key metrics (errors, latency, availability)"
```

---

## Summary

**GCP Services by Use Case:**

| Use Case | Recommended Service |
|----------|-------------------|
| Web Apps | Cloud Run / App Engine |
| Containers | GKE |
| VMs | Compute Engine |
| Database (SQL) | Cloud SQL |
| Database (NoSQL) | Firestore / Bigtable |
| Data Warehouse | BigQuery |
| File Storage | Cloud Storage |
| Cache | Memorystore (Redis/Memcached) |
| Async Jobs | Cloud Tasks / Pub/Sub |
| Scheduled Jobs | Cloud Scheduler |
| CI/CD | Cloud Build |
| Secrets | Secret Manager |
| Keys | Cloud KMS |
| Networking | VPC / Cloud Load Balancing |

---

## GCP Service Selection Decision Tree

**Quick Reference for Service Selection:**

```
Need to run code?
├─ YES
│  └─ How long does it run?
│     ├─ < 15 minutes
│     │  └─ Cloud Functions (simplest)
│     ├─ 15 minutes to 1 hour
│     │  └─ Cloud Run (containers)
│     └─ > 1 hour or always running
│        └─ Compute Engine or GKE
│
└─ NO (just store/analyze data)
   └─ What's the data type?
      ├─ Structured (SQL)
      │  └─ Cloud SQL or Spanner
      ├─ Documents/JSON
      │  └─ Firestore
      ├─ Time-series/billions of points
      │  └─ Bigtable
      ├─ Analytics/BI queries
      │  └─ BigQuery
      └─ Files/objects
         └─ Cloud Storage

Need to process data?
├─ Real-time (< 1 second)
│  └─ Cloud Pub/Sub + Cloud Run
├─ Batch (minutes to hours)
│  └─ Dataflow or BigQuery
└─ Complex pipelines
   └─ Dataflow (Apache Beam)
```

### Common GCP Misconceptions & Corrections

```
MYTH 1: "Cloud is always cheaper than on-premises"
REALITY: ├─ True for variable workloads (bursty traffic)
         ├─ False for stable workloads (24/7 same load)
         ├─ Break-even at 3-5 years (cloud wins after)
         └─ Key: Commitments make cloud cheaper

MYTH 2: "Serverless has no costs when idle"
REALITY: ├─ True for most serverless (Cloud Run, Functions)
         ├─ False if you keep warm instances (costs money)
         ├─ Concurrent connections cause charges
         └─ Always check pricing before assuming free

MYTH 3: "Multi-region = high availability"
REALITY: ├─ Multi-region = disaster recovery (hours to days)
         ├─ Multi-zone = high availability (seconds)
         ├─ Confusion common in interviews
         └─ Need both for production: multi-zone + multi-region

MYTH 4: "Bigger instance = always better"
REALITY: ├─ Bigger instance = fewer instances to manage
         ├─ But less horizontal scaling flexibility
         ├─ Optimal: Medium instances with 3-5 in group
         └─ Vertical scaling has limits

MYTH 5: "Cloud SQL is always better than self-hosted"
REALITY: ├─ True: Backups, patches, replicas automatic
         ├─ False: Can be more expensive than Compute Engine
         ├─ False: Less customization (can't tune OS)
         └─ Key: Managed cost offset by operational savings

MYTH 6: "Firewall is all you need for security"
REALITY: ├─ Firewall = network security (perimeter)
         ├─ Also need: IAM, encryption, audit logs
         ├─ Defense in depth: Multiple layers
         └─ Single layer = false sense of security
```

### GCP Best Practices Summary

```
MUST DO:
├─ Use service accounts (never personal accounts in automation)
├─ Enable audit logging (compliance & investigation)
├─ Use IAM roles (not basic Owner/Editor/Viewer)
├─ Enable VPC (isolate resources)
├─ Use Cloud KMS for encryption keys (not hardcoded)
├─ Multi-zone for production (automatic failover)
└─ Monitoring & alerting (catch problems early)

SHOULD DO:
├─ Infrastructure as Code (Terraform, Cloud Deployment Manager)
├─ Secrets in Secret Manager (not in code/config)
├─ Read replicas for databases (handle read load)
├─ Cloud CDN for static content (faster, cheaper)
├─ Auto-scaling (elastic resource allocation)
├─ Committed use discounts (save 30-70% if predictable)
└─ Backup strategy (test recovery procedures)

NICE TO DO:
├─ VPC Service Controls (extra security)
├─ Workload Identity (no keys to rotate)
├─ Cloud Trace (distributed tracing)
├─ Cloud Profiler (performance optimization)
├─ Reserved capacity (guaranteed capacity)
└─ Machine images (faster deployment)

AVOID AT ALL COSTS:
├─ Basic roles (Editor, Owner, Viewer)
├─ Storing secrets in code
├─ Single zone deployment for production
├─ Not monitoring (blind to problems)
├─ Unused resources (drain on budget)
├─ Manual infrastructure (error-prone, non-repeatable)
├─ No backup/recovery plan
└─ Ignoring audit logs (can't investigate breaches)
```

---

## Troubleshooting & Debugging

### Common GCP Issues and Solutions

#### Issue 1: Cloud Run Service Timing Out

**Symptoms:**
```
503 Service Unavailable
Error: Deadline exceeded
Logs: Container terminated before responding to requests
```

**Root Cause Analysis:**
```
Possible causes (in order of likelihood):
1. Database connection timeout (Cold start + slow query)
2. External API call hanging
3. Memory exhaustion (OOM)
4. CPU throttling (using preemptible instances)
5. Request timeout < actual execution time
```

**Debugging Steps:**
```bash
# 1. Check recent deployments
gcloud run revisions list --service=my-service

# 2. Check logs
gcloud run logs read my-service --limit=50

# Look for patterns in logs:
# - "database connection timeout" → DB issue
# - "Memory exhausted" → Increase memory allocation
# - "Request timeout" → Increase timeout setting

# 3. Test locally with larger timeout
curl --max-time 120 https://my-service.run.app

# 4. Check resource allocation
gcloud run services describe my-service \
    --format='value(spec.template.spec.containers[0].resources)'

# 5. Monitor metrics
gcloud monitoring read \
    --filter 'resource.labels.service_name="my-service"' \
    --start-time=-1h
```

**Solution:**
```bash
# Increase memory and CPU
gcloud run deploy my-service \
    --memory 2Gi \
    --cpu 2 \
    --timeout 120 \
    --max-instances 100

# Optimize application
# 1. Connection pooling
# 2. Query optimization
# 3. Caching frequently accessed data
# 4. Async processing for long-running tasks
```

#### Issue 2: GKE Node Running Out of Resources

**Symptoms:**
```
Pods stuck in Pending state
Events: FailedScheduling - Insufficient memory
Some pods evicted after being Pending
```

**Debugging:**
```bash
# 1. Check node status
kubectl get nodes
kubectl describe node NODE_NAME

# 2. Check pod resources
kubectl top nodes
kubectl top pods --all-namespaces

# 3. Look for resource limits
kubectl get pods --all-namespaces \
    -o custom-columns=NAME:.metadata.name,\
MEMORY_LIMIT:.spec.containers[0].resources.limits.memory,\
CPU_LIMIT:.spec.containers[0].resources.limits.cpu

# 4. Check if auto-scaling is enabled
gcloud container clusters describe CLUSTER_NAME \
    --format='value(autoscaling.enableNodeAutoscaling)'

# 5. Monitor cluster capacity
kubectl get nodes -o wide | awk '{print $1, $6, $7}'
```

**Solution:**
```bash
# 1. Enable/verify auto-scaling
gcloud container clusters update CLUSTER_NAME \
    --enable-autoscaling \
    --min-nodes 3 \
    --max-nodes 20 \
    --zone=us-central1-a

# 2. Right-size pod requests
# Update deployment with appropriate resource requests
kubectl set resources deployment/my-app \
    --requests=memory=256Mi,cpu=250m \
    --limits=memory=512Mi,cpu=500m

# 3. Add node pool for high-memory workloads
gcloud container node-pools create high-memory \
    --cluster=CLUSTER_NAME \
    --machine-type=n1-highmem-4 \
    --num-nodes=2

# 4. Use Pod Disruption Budgets for critical apps
```

#### Issue 3: Cloud SQL High Latency

**Symptoms:**
```
Queries taking 500ms+
Application timeout errors
CPU usage normal but queries slow
```

**Debugging:**
```bash
# 1. Check Query Insights (built-in)
gcloud sql operations list --instance=INSTANCE_NAME

# 2. Enable slow query logging
gcloud sql instances patch INSTANCE_NAME \
    --database-flags=slow_query_log=on,long_query_time=1

# 3. Check slow query log
gcloud sql operations list INSTANCE_NAME

# 4. Monitor metrics
gcloud monitoring read \
    --filter 'resource.type="cloudsql_database"' \
    --format='table(metric.type, points[0].value.double_value)' \
    --start-time=-1h

# 5. Analyze specific queries
# Connect to database
gcloud sql connect INSTANCE_NAME

# Run EXPLAIN ANALYZE
EXPLAIN ANALYZE SELECT * FROM large_table WHERE indexed_column = 'value';
```

**Solution:**
```sql
-- Add missing indexes
CREATE INDEX idx_user_email ON users(email);
CREATE INDEX idx_order_created ON orders(created_at);

-- Optimize query
-- Bad: SELECT * (fetches all columns)
-- Good: SELECT id, name, email (fetch only needed columns)

-- Use LIMIT
-- Bad: SELECT * FROM orders (millions of rows)
-- Good: SELECT * FROM orders LIMIT 100 OFFSET 0 (pagination)

-- Use covering indexes
CREATE INDEX idx_user_email_name ON users(email) INCLUDE (name, created_at);
```

---

## Performance Tuning

### Optimizing Cloud Run Cold Starts

**Problem:**
```
First request after deployment: 5 seconds
Subsequent requests: 100ms
Users notice slow initial load
```

**Solutions (in order of impact):**

```python
# 1. Minimize container size
# Use distroless base image
# FROM python:3.11  (900MB)
# FROM python:3.11-slim  (300MB)
# FROM gcr.io/distroless/python3  (100MB)

# 2. Pre-initialize expensive resources
# Bad: Initialize on every request
@app.route('/api/data')
def get_data():
    client = bigquery.Client()  # Slow!
    return query_data(client)

# Good: Initialize once at startup
bq_client = bigquery.Client()  # Startup: slow, but only once

@app.route('/api/data')
def get_data():
    return query_data(bq_client)

# 3. Use minimum memory, increase CPU
# More CPU = faster runtime (included in memory tier)
gcloud run deploy --memory 256Mi  # 0.08 vCPU
gcloud run deploy --memory 2Gi    # 2 vCPU (20x faster)

# 4. Keep Cloud Run service warm
# Schedule requests every 10 minutes
# from Google Cloud Scheduler
gcloud scheduler jobs create http keep-warm \
    --location=us-central1 \
    --schedule="*/10 * * * *" \
    --uri=https://my-service.run.app/health \
    --http-method=GET

# 5. Use Python 3.11+ (faster startup than 3.9)

# 6. Remove unused dependencies
# Check imports, uninstall unused packages
```

### Optimizing BigQuery Costs

**Problem:**
```
Running same queries multiple times = 100x cost
10 dashboards querying same data
```

**Solutions:**

```sql
-- 1. Use cached results (saves money!)
-- Query runs once, results cached for 24 hours
SELECT * FROM `my-project.dataset.users` WHERE created_at > DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)

-- 2. Partition tables (pay for only relevant data)
-- Without partition: scanning 1TB = $6.25
-- With partition on date: scanning 1 day = $0.02

CREATE TABLE my_project.dataset.events (
  event_id STRING,
  timestamp TIMESTAMP,
  user_id STRING
)
PARTITION BY DATE(timestamp)
CLUSTER BY user_id;

-- 3. Use clustering (especially for wide tables)
CREATE TABLE my_project.dataset.customer_data (
  customer_id STRING,
  purchase_date DATE,
  amount DECIMAL64(10, 2),
  ...1000 columns
)
CLUSTER BY customer_id, purchase_date;

-- 4. Sample data for exploration
-- Don't query entire dataset when exploring
SELECT * FROM `my-project.dataset.events` 
WHERE RAND() < 0.01  -- Only 1% of rows, 99% savings
LIMIT 10000;

-- 5. Use approximate functions
-- APPROX_COUNT_DISTINCT (faster, uses ~10% data)
-- Instead of COUNT(DISTINCT ...)
SELECT 
  APPROX_COUNT_DISTINCT(user_id) as users,  -- Fast
  COUNT(DISTINCT user_id) as exact_users    -- Slow
FROM events;

-- 6. Denormalize data (avoid expensive JOINs)
-- Instead of joining 5 tables, use pre-calculated denormalized table
-- Update batch nightly, query fast during day

-- 7. Use BI Engine (in-memory caching)
-- First 100GB free (if instance < 100GB)
SELECT * FROM `my-project.dataset.events`  -- Cached in memory
WHERE user_id = '123'  -- Sub-second response
```

### Optimizing GKE Costs

**Problem:**
```
Paying for idle node capacity
Multiple node pools under-utilized
Keeping nodes warm 24/7 for peak hours (only happens 1 hour/day)
```

**Solutions:**

```yaml
# 1. Use Autopilot (no node management needed)
# GCP optimizes resources automatically
gcloud container clusters create my-cluster --enable-autopilot

# 2. Enable Pod Disruption Budgets (use preemptible VMs more)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: my-app

---
# Use preemptible nodes (80% cheaper)
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  nodeSelector:
    cloud.google.com/gke-preemptible: "true"
  containers:
  - name: app
    image: my-app:v1

# 3. Use Workload Identity instead of service account keys
# No extra charges, more secure
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  annotations:
    iam.gke.io/gcp-service-account: app@my-project.iam.gserviceaccount.com

---
# 4. Right-size requests/limits
# Bad: requests: cpu: 1000m, limits: cpu: 2000m
# Good: requests: cpu: 100m, limits: cpu: 500m (90% savings)

# 5. Use Horizontal Pod Autoscaling aggressively
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 1      # Scale down to 1
  maxReplicas: 50     # Scale up aggressively
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # Aggressive scaling

# 6. Schedule batch jobs during off-peak (cheaper)
# Run heavy jobs at 2 AM (no users online)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: batch-job
spec:
  schedule: "0 2 * * *"  # 2 AM daily
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: job
            image: my-batch-job:v1
          restartPolicy: Never
```

---

## Common Patterns & Anti-Patterns

### Pattern 1: Graceful Degradation

**Scenario:** One microservice is down, but others should keep working

```python
# Good: Fallback when service fails
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=60)
def call_recommendation_service(user_id):
    """Call recommendation service with circuit breaker"""
    response = requests.get(
        f'http://recommendation-service/api/recommendations/{user_id}',
        timeout=2
    )
    return response.json()

@app.route('/api/products/<product_id>')
def get_product(product_id):
    product = get_product_from_db(product_id)
    
    try:
        # Try to get recommendations (might be slow/down)
        product['recommendations'] = call_recommendation_service(user_id)
    except Exception as e:
        # Service down? Return without recommendations
        logger.warning(f"Recommendation service failed: {e}")
        product['recommendations'] = []  # Fallback
    
    return jsonify(product)
```

### Anti-Pattern: Direct Synchronous Calls

```python
# Bad: All services called synchronously
# If recommendation service slow: entire response slow
def get_product(product_id):
    product = get_product_from_db(product_id)
    product['recommendations'] = call_recommendation_service(product_id)  # SLOW!
    product['reviews'] = call_review_service(product_id)  # SLOW!
    product['inventory'] = call_inventory_service(product_id)  # SLOW!
    return jsonify(product)  # Entire response blocked

# Good: Async/parallel calls
async def get_product_async(product_id):
    product = await get_product_from_db(product_id)
    
    # Run all in parallel
    recommendations, reviews, inventory = await asyncio.gather(
        call_recommendation_service_async(product_id),
        call_review_service_async(product_id),
        call_inventory_service_async(product_id)
    )
    
    product['recommendations'] = recommendations
    product['reviews'] = reviews
    product['inventory'] = inventory
    return product
```

### Pattern 2: Event-Driven Architecture

```python
# Good: Loosely coupled services via events
# Service A publishes event, Service B reacts
# No direct dependency

# Service A: Order Service
def create_order(user_id, items):
    order = Order.create(user_id=user_id, items=items)
    
    # Publish event (fire and forget)
    publisher.publish('order-created', {
        'order_id': order.id,
        'user_id': user_id,
        'items': items,
        'total': order.total
    })
    
    return order

# Service B: Inventory Service (listens to 'order-created')
def on_order_created(event):
    order_id = event['order_id']
    items = event['items']
    
    # Update inventory asynchronously
    for item in items:
        decrease_stock(item['product_id'], item['quantity'])
    
    logger.info(f"Inventory updated for order {order_id}")

# Service C: Notification Service (also listens to 'order-created')
def on_order_created(event):
    order_id = event['order_id']
    user_id = event['user_id']
    
    # Send email asynchronously
    send_order_confirmation_email(user_id, order_id)
```

### Anti-Pattern: Synchronous Cascading Calls

```python
# Bad: Each service calls next service
# If any service slow: entire chain blocks
# Tight coupling

def create_order(user_id, items):
    # Create order
    order = Order.create(user_id=user_id, items=items)
    
    # Synchronously wait for inventory update
    inventory_response = requests.post(
        'http://inventory-service/api/update-stock',
        json={'order_id': order.id, 'items': items}
    )
    
    # Synchronously wait for notification
    notification_response = requests.post(
        'http://notification-service/api/send-email',
        json={'user_id': user_id, 'order_id': order.id}
    )
    
    return order  # Only returns after ALL services done
```

### Pattern 3: Caching Strategy

```python
# Good: Multi-level caching
# L1: Memory (local, fast)
# L2: Redis (shared, fast)
# L3: Database (slow)

import functools
import time

class CacheManager:
    def __init__(self, redis_client, local_ttl=60, redis_ttl=300):
        self.redis = redis_client
        self.local_cache = {}  # Memory cache
        self.local_ttl = local_ttl  # 1 minute
        self.redis_ttl = redis_ttl  # 5 minutes
    
    def cached(self, key_prefix):
        def decorator(func):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # Build cache key
                key = f"{key_prefix}:{args}:{kwargs}"
                
                # L1: Check local memory
                if key in self.local_cache:
                    value, timestamp = self.local_cache[key]
                    if time.time() - timestamp < self.local_ttl:
                        logger.info(f"L1 cache hit: {key}")
                        return value
                
                # L2: Check Redis
                redis_value = self.redis.get(key)
                if redis_value:
                    value = json.loads(redis_value)
                    self.local_cache[key] = (value, time.time())
                    logger.info(f"L2 cache hit: {key}")
                    return value
                
                # L3: Call function
                logger.info(f"Cache miss, computing: {key}")
                value = func(*args, **kwargs)
                
                # Store in both caches
                self.redis.setex(key, self.redis_ttl, json.dumps(value))
                self.local_cache[key] = (value, time.time())
                
                return value
            
            return wrapper
        return decorator

# Usage
cache_manager = CacheManager(redis_client)

@cache_manager.cached('user')
def get_user(user_id):
    """Expensive database query"""
    return db.query(f"SELECT * FROM users WHERE id = {user_id}")
```

### Anti-Pattern: Cache Stampede

```python
# Bad: Cache expires, everyone queries database simultaneously
# All concurrent requests call database (thundering herd)

cache_key = 'expensive_computation'

def get_data():
    # Cache expired, compute again
    if cache.get(cache_key) is None:
        # Multiple requests hit this simultaneously
        result = expensive_computation()  # Database hit!
        cache.set(cache_key, result, ttl=3600)
    
    return cache.get(cache_key)

# Good: Use probabilistic early expiration
import random

def get_data():
    cached = cache.get(cache_key)
    
    if cached:
        # Cache still valid, but regenerate probabilistically
        ttl = cache.ttl(cache_key)
        
        if ttl < 60 and random.random() < 0.1:
            # 10% of requests refresh cache early
            result = expensive_computation()
            cache.set(cache_key, result, ttl=3600)
        
        return cached
    else:
        # Expired, compute
        result = expensive_computation()
        cache.set(cache_key, result, ttl=3600)
        return result
```

---

## Advanced GCP Concepts

### Understanding GCP Resource Quotas

```
Each GCP project has quotas (limits) on resources:

API Quotas:
├─ Cloud Run invocations: 50M per day (free tier)
├─ Cloud Build minutes: 120 per day (free tier)
├─ Cloud Functions invocations: 2M per month (free tier)
├─ Pub/Sub publish requests: 50K per day (free tier)
└─ BigQuery queries: No hard limit (quota by region)

Compute Quotas:
├─ CPU cores: Usually 12-100 (varies by region)
├─ Memory: Usually 40GB (varies by region)
├─ GPUs: Usually 0 (request quota increase)
└─ Disks: 500GB (varies by type)

Network Quotas:
├─ VPC networks: 5 per project
├─ Subnets: 300 per VPC
├─ Firewall rules: 256 per VPC
└─ Routes: 1000 per VPC

Storage Quotas:
├─ Cloud Storage buckets: 300 per project (soft)
├─ BigQuery datasets: No hard limit
└─ Cloud SQL instances: Usually 10 (request increase)

How to Check Quotas:
$ gcloud compute project-info describe --project=PROJECT_ID

How to Request Quota Increase:
Google Cloud Console → Quotas → Select quota → Edit quota → Request

Typical Request: "Need 200 CPU cores for Kubernetes cluster"
Processing time: 24-48 hours
```

### VPC Service Controls Deep Dive

```
Problem: Sensitive data (PII, financial) needs protection
Solution: VPC Service Controls creates "security perimeter"

Architecture:
┌─────────────────────────────────┐
│   Security Perimeter             │
│                                 │
│  ┌─ Cloud Storage               │ (encrypted, isolated)
│  ├─ BigQuery                    │ (no export outside)
│  ├─ Cloud SQL                   │ (no public IP)
│  └─ Firestore                   │ (restricted access)
│                                 │
│  Access Policy:                 │
│  ├─ Allow only GKE pods         │ (with service account)
│  ├─ Allow only specific VPC     │
│  └─ Deny all other access       │
│                                 │
└─────────────────────────────────┘

Real-world example:
HIPAA-regulated healthcare app
├─ Patient data in BigQuery (inside perimeter)
├─ Backend service in GKE (inside perimeter)
├─ Mobile app outside (cannot directly access)
├─ Data flows: Mobile → API → GKE → BigQuery (inside perimeter)
└─ If any component breached: Data still protected by perimeter
```

### Workload Identity Federation

```
Problem: Application needs AWS credentials to access S3
Traditional: Store AWS keys in secret → Risk of leak

Solution: Workload Identity Federation
├─ GCP service account + Kubernetes RBAC
├─ No keys to manage
├─ Automatic credential rotation
└─ Audit trail: Who accessed what

Setup:
1. Create GCP service account
2. Create Kubernetes service account
3. Bind them (Workload Identity)
4. Pod gets automatic credentials

No manual key management!

Example:
$ gcloud iam service-accounts create my-app

$ kubectl create serviceaccount my-app-sa

$ gcloud iam service-accounts add-iam-policy-binding \
    my-app@my-project.iam.gserviceaccount.com \
    --role roles/iam.workloadIdentityUser \
    --member "serviceAccount:my-project.svc.id.goog[default/my-app-sa]"

# Pod automatically gets credentials
# No GOOGLE_APPLICATION_CREDENTIALS needed
```

---

## Debugging Tools & Techniques

### Using gcloud CLI Effectively

```bash
# 1. Configure default project
gcloud config set project my-project

# 2. List all resources in project
gcloud compute instances list               # VMs
gcloud sql instances list                   # Databases
gcloud container clusters list              # Kubernetes clusters
gcloud run services list                    # Cloud Run services
gcloud functions list                       # Cloud Functions
gcloud storage buckets list                 # Cloud Storage

# 3. Debug a failing service
gcloud run services describe my-service     # See configuration
gcloud run logs read my-service --limit=100  # Last 100 log lines

# 4. SSH into a VM
gcloud compute ssh INSTANCE_NAME --zone=us-central1-a

# 5. Port forward to local machine
gcloud compute ssh INSTANCE_NAME \
    --zone=us-central1-a \
    --tunnel-through-iap \
    -- -L 3306:localhost:3306

# Now: mysql -h localhost (connects to remote MySQL via tunnel)

# 6. Check authentication
gcloud auth list              # Who am I?
gcloud auth print-access-token  # Current token
gcloud auth application-default login  # For applications

# 7. Monitor in real-time
watch -n 1 'gcloud compute instances list'

# 8. Export/Import configurations
gcloud deployment-manager deployments describe DEPLOYMENT --format json > config.json
```

### Using Cloud Logging Effectively

```bash
# 1. View logs
gcloud logging read "resource.type=cloud_run_revision AND severity=ERROR"

# 2. Stream logs (like tail -f)
gcloud logging read "resource.type=cloud_function" --follow

# 3. Filter by time
gcloud logging read "severity=ERROR" --freshness=1h

# 4. Export logs
gcloud logging read "severity=ERROR" > errors.txt

# 5. Filter by label
gcloud logging read "labels.service=payment-service" --limit=50

# 6. Complex queries
gcloud logging read """
resource.type="cloud_run_revision" AND
  (severity="ERROR" OR httpRequest.status >= 500) AND
  timestamp >= "2024-12-24T10:00:00Z"
""" --limit=100 --format=json | jq '.[] | {timestamp, message: .textPayload}'
```

### Monitoring and Debugging Kubernetes

```bash
# 1. Check pod status
kubectl get pods                    # All pods
kubectl get pods -A                 # All pods, all namespaces
kubectl describe pod POD_NAME        # Details
kubectl logs POD_NAME                # Logs

# 2. Check events (what happened?)
kubectl get events --all-namespaces  # All events
kubectl describe node NODE_NAME      # Node events

# 3. Resource usage
kubectl top nodes          # Node CPU/memory
kubectl top pods           # Pod CPU/memory

# 4. Network debugging
kubectl exec POD_NAME -- curl http://other-service:8080  # Test connection

# 5. Port forward for debugging
kubectl port-forward POD_NAME 8080:8080
# Now: curl localhost:8080

# 6. Debug pod that crashes immediately
kubectl run -it --image=my-image debug /bin/bash
# Test image manually

# 7. Check persistent volumes
kubectl get pv              # Persistent volumes
kubectl get pvc             # Claims
kubectl describe pvc        # Details
```

---

## Interview Scenarios - DevOps & Cloud Engineer Roles

### Q1: Design a Scalable Web Application on GCP

**Scenario:**
```
You're asked to design infrastructure for:
├─ E-commerce website
├─ Expected: 10,000 concurrent users initially
├─ Peak: 100,000 concurrent users during sales
├─ Geographic: Global audience
├─ Requirements:
│  ├─ 99.9% uptime SLA
│  ├─ Sub-second latency
│  ├─ Handle traffic spikes
│  └─ Minimize costs
└─ Interview time: 30-45 minutes (whiteboard/diagram)
```

**What Interviewer is Looking For:**
```
✓ Understand user traffic patterns
✓ Know when to scale vertically vs horizontally
✓ Geographic distribution for latency
✓ Cost awareness (committed discounts, preemptible VMs)
✓ Security considerations (no exposed databases)
✓ Monitoring and alerting strategy
✗ Avoid: Over-engineering, single point of failure
```

**Your Answer (Structure):**

**Layer 1: Global Distribution**
```
Use Global HTTPS Load Balancer + Cloud CDN
├─ Serves static content from Google POPs globally
├─ Reduces latency for users worldwide
└─ Reduces origin load by 50-80%
```

**Layer 2: Compute (API Backend)**
```
Use GKE (Kubernetes) with Autopilot
├─ Why Kubernetes?
│  ├─ Easy scaling (horizontal pod autoscaling)
│  ├─ Rolling updates (zero downtime deployments)
│  ├─ Self-healing (replaces failed pods)
│  └─ Multi-zone deployment (high availability)
├─ Regional cluster (nodes across 3 zones)
├─ Minimum 3 nodes, maximum 100 nodes (auto-scaling)
├─ Pod replicas: 3-50 based on CPU/memory metrics
└─ Cost: ~$150-$200/month (Autopilot manages nodes)
```

**Layer 3: Database**
```
Use Cloud SQL with High Availability
├─ Multi-zone replication (automatic failover)
├─ Read replica in different region (for analytics)
├─ Size: db-n1-standard-2 (should handle 10K concurrent)
├─ Backup: Daily + point-in-time recovery
├─ Connection pooling: Use Cloud SQL Proxy
└─ Cost: ~$500-$800/month
```

**Layer 4: Cache**
```
Use Memorystore (Redis)
├─ Session storage
├─ Frequently accessed data (products, categories)
├─ Rate limiting counter
├─ High availability mode (automatic failover)
└─ Cost: ~$200-$300/month
```

**Layer 5: Static Content**
```
Use Cloud Storage + Cloud CDN
├─ Images, CSS, JavaScript
├─ Lifecycle policy: Archive old files after 90 days
├─ Versioning: Automatic cache busting in code
└─ Cost: ~$50-$100/month
```

**Scaling Strategy:**
```
Under 10,000 users:
├─ 3 GKE nodes (1 per zone)
├─ 3-5 pod replicas
├─ db-n1-standard-2 Cloud SQL
├─ 1 Memorystore instance
└─ Total cost: ~$1,000/month

During 100,000 peak:
├─ 20 GKE nodes (auto-scaled)
├─ 30-50 pod replicas (auto-scaled)
├─ Same Cloud SQL (has capacity)
├─ Same Memorystore (has capacity)
└─ Total cost: ~$2,000/month (temporary)
```

**Monitoring & Alerting:**
```
Dashboards:
├─ Request rate (requests/second)
├─ Error rate (5xx errors)
├─ Latency (P50, P95, P99)
├─ Database connections
└─ Pod restart count

Alerts:
├─ Error rate > 1% (page on-call)
├─ P99 latency > 2s (email/slack)
├─ Database CPU > 80% (scale up needed)
└─ Cluster autoscaler at max (out of capacity)
```

**Interviewer Follow-up:**

Q: "What if database becomes bottleneck?"
A: Optimize queries → Add indexes → Read replicas → Vertical scale (last resort)

Q: "How do you do zero-downtime deployments?"
A: Kubernetes rolling updates → New pod version starts → Health checks pass → Load balancer routes new traffic → Old pod drains connections → No user requests dropped

---

### Q2: Incident Response - Production Database Down

**Scenario:**
```
Friday 2 PM: Production database stops responding
├─ Alerts: Database CPU 100%, connections maxed
├─ Users: Experiencing slow loading
├─ Team: 3 engineers on call
```

**Timeline of Response:**

**T=0:00 (First 30 seconds)**
```
├─ Check monitoring dashboard
├─ Database CPU: 95%, Connections: 450/500 maxed
├─ Last change: Deployment 30 min ago
└─ Hypothesis: Expensive query from new code
```

**T=0:05 (Immediate action)**
```
├─ Identify slow query from logs
├─ Kill the slow query: KILL QUERY PID;
├─ CPU drops to 40%, connections drop to 150/500
└─ App becomes responsive again
```

**T=0:15 (Verify recovery)**
```
├─ Application working (P99 latency < 1s)
├─ Monitor for stability
└─ No customer impact (incident only 15 minutes)
```

**T=1:00 (Root cause fix)**
```
├─ New query missing index on timestamp column
├─ Full table scan: 100M rows × 5 seconds = timeout
├─ Solution: CREATE INDEX idx_timestamp ON events(timestamp);
├─ Deploy with code optimization (add user_id filter)
└─ Load test with production data volume before next deployment
```

**Prevention for future:**
```
├─ Add slow query alert (> 2 seconds)
├─ Require EXPLAIN ANALYZE in code reviews
├─ Load test with production-sized data
└─ Query review checklist in PR template
```

---

### Q3: Design Disaster Recovery Plan

**Scenario:**
```
"What if entire us-central1 region fails?"
├─ RPO: 5 minutes (can lose 5 min of data)
├─ RTO: 30 minutes (must be back up)
├─ Budget: $5K/month additional
```

**Answer:**

**Primary (Active) + Backup (Standby) Setup:**
```
Primary: us-central1 ($1,200/month)
├─ GKE: 5-10 nodes
├─ Cloud SQL: HA setup
├─ Memorystore: Redis

Backup: us-east1 ($300/month)
├─ GKE: Minimal (2 nodes, scales on failover)
├─ Cloud SQL: Read replica
├─ Memorystore: Async replica
```

**Failover (Automated):**
```
Detection: Uptime check fails 3x (30 seconds)
   ↓
Promotion: Read replica becomes primary (5 min)
   ↓
DNS: Switch to backup region (1-2 min)
   ↓
Total RTO: ~15 minutes ✓
Total cost: $1,500/month (within $5K budget)
```

**Monthly DR Drill:**
```
├─ Simulate primary region failure
├─ Measure: RTO, data loss, user impact
├─ Fix any issues
└─ Yearly: Full region failure test
```

---

### Q4: Optimize Cloud Spend by 40%

**Scenario:**
```
GCP bill: $50,000/month
Target: Reduce by 40% ($20,000 savings)
Timeline: 3 months
```

**Quick Wins ($11,500/month savings):**

```
1. Compute Optimization ($5,000)
   ├─ Kill unused VMs: $1,500
   ├─ Reduce minimum Cloud Run instances: $3,000
   └─ Right-size GKE nodes: $500

2. Storage Optimization ($4,000)
   ├─ Delete old snapshots: $1,000
   ├─ Archive old logs to Coldline: $800
   ├─ Reduce Cloud SQL backup retention: $2,000
   └─ Remove unused databases: $200

3. Network Optimization ($2,500)
   ├─ Enable Cloud CDN: $500
   ├─ Consolidate databases to single region: $1,500
   └─ Compress logs: $500
```

**Long-Term Savings ($10,000+/month):**

```
├─ 1-year CUD (Compute, Cloud SQL): $6,000/month
├─ Preemptible VMs for batch jobs: $2,000/month
├─ Architecture optimization: $2,000/month
└─ Total 3-month result: 43% reduction ($28,500/month bill)
```

**Monitoring Going Forward:**
```
├─ Weekly dashboard (cost trends)
├─ Alerts (cost spike > 10%)
├─ Annual review (renegotiate commitments)
└─ Automated resource cleanup
```

---

### Q5: Kubernetes Memory Crisis (OOMKilled Pods)

**Scenario:**
```
Monday 9 AM:
├─ Alert: Node pool 98% memory
├─ Problem: 20 pods OOMKilled overnight
├─ Impact: 404 errors for users
```

**Response Timeline:**

**T=0:00 (Immediate)**
```
├─ kubectl get nodes (check capacity)
├─ Find OOMKilled pods
├─ Scale down non-critical services (free memory)
└─ Action: kubectl scale deployment analytics --replicas=0
```

**T=0:05 (Investigation)**
```
├─ analytics-worker: Loading all user history in memory
├─ Testing: 10 users worked, production 10K users → 9GB needed
├─ Not detected: No memory monitoring, untested at scale
```

**T=0:20 (Immediate Fix)**
```
├─ Add 5 more nodes (quick relief, $100/month)
├─ Update pod memory requests (2Gi → 4Gi)
├─ Pods gradually restart
└─ After 10 min: Memory back to 60%, all pods healthy
```

**T=2:00 (Restore Services)**
```
└─ Scale back analytics services (memory now available)
```

**Permanent Fix:**
```
├─ Implement cache eviction (keep only 24h of data)
├─ Add resource requests/limits (scheduler won't overpack)
├─ Add memory alerts (pod > 80% limit)
├─ Memory monitoring dashboard
└─ Deploy with blue-green (verify no issues)
```

**Post-Incident:**
```
├─ Document runbook: "Pod OOMKilled diagnosis"
├─ Code review checklist: Memory assumptions
├─ Load test requirements: Test at production scale
├─ Standard pod template: Always include limits
└─ Training: Debugging memory in Kubernetes
```

---

### Q6: Infrastructure as Code Migration (Terraform)

**Scenario:**
```
Current: 50+ resources created manually in console
Problem: No version control, can't recreate if needed
Goal: Migrate to Terraform IaC (zero downtime)
Timeline: 3 months
```

**Phase 1: Planning (Week 1-2)**

```
1. Inventory all resources
   ├─ 25 Compute Engine instances
   ├─ 5 Cloud SQL databases
   ├─ 8 Cloud Storage buckets
   ├─ 2 GKE clusters
   ├─ 40 firewall rules
   └─ 200+ IAM bindings

2. Setup infrastructure
   ├─ Git repo (private, version control)
   ├─ Remote state (GCS with versioning & locking)
   └─ CI/CD pipeline (validate, plan, review, apply)
```

**Phase 2: Development (Week 3-8)**

```
Sprint 1: VPC & Networking
├─ Create modules: VPC, subnets, firewall
├─ Import existing resources (no changes yet)
└─ Verify: terraform plan shows no changes

Sprint 2: Compute Resources
├─ Create new instances via Terraform
├─ Run parallel with old ones for 1 week
├─ If stable: Kill old ones
└─ Zero downtime (both versions running)

Sprint 3: Databases & Storage
├─ Import existing databases (terraform import)
├─ Import existing storage buckets
├─ Backup first (safety net)
└─ Now Terraform manages them
```

**Phase 3: Testing (Week 9-10)**

```
1. Staging Environment
   ├─ Destroy existing staging
   ├─ Create fresh from Terraform
   ├─ Deploy app, run integration tests
   └─ Verify performance (same as before)

2. Production Validation
   ├─ terraform plan (should be empty or approved changes)
   ├─ Verify all 50 resources in Terraform
   └─ Disaster recovery test (delete resource, can Terraform recreate?)
```

**Phase 4: Finalization (Week 11-12)**

```
1. Lock Down
   ├─ Only "terraform" service account can modify
   ├─ Console modifications disabled (IAM policy)
   └─ Emergency access: Break glass admin account

2. Documentation
   ├─ README: How to deploy
   ├─ Runbook: Common operations (scale, add environment, update DB)
   └─ Team training: PR review, common mistakes, state recovery

3. Directory Structure
   terraform/
   ├─ modules/ (reusable components)
   │  ├─ vpc/
   │  ├─ compute/
   │  ├─ gke/
   │  ├─ cloud-sql/
   │  └─ storage/
   ├─ prod/ (production resources)
   ├─ staging/ (staging resources)
   └─ dev/ (development resources)
```

**Handling Databases (Critical):**
```
Import existing database:
├─ terraform import google_sql_database_instance.prod \
   my-existing-database
├─ Backup database first (safety net)
├─ Verify: terraform plan shows no changes
└─ Now Terraform manages it

Secret management:
├─ Never store passwords in git
├─ Use Google Secret Manager
├─ Reference in Terraform via data source
└─ IAM: Only terraform service account can read
```

**Interviewer Follow-up:**

Q: "What if terraform.tfstate gets corrupted?"
A: Store in GCS with versioning → Restore from old version. Encrypt with Cloud KMS. Use state locking to prevent concurrent edits.

Q: "How do you handle secrets?"
A: Use Google Secret Manager (never git). Reference via data source. IAM: Only terraform service account reads. Rotation: Automatic.

Q: "How do you revert a deployment?"
A: Git revert commit → terraform plan (verify) → terraform apply. Always plan before apply. Code review catches mistakes. Staging test first.

---

## Interview Tips

### Before Interview

**Preparation (2-3 weeks before)**
- Study 5-10 GCP architecture patterns
- Practice whiteboard diagrams (use draw.io or paper)
- Read GCP case studies from Google Cloud documentation
- Time yourself on scenario questions (30-45 minutes)

**Day Before**
- Get good sleep (better than cramming)
- Review "60 second summary" of GCP services
- Prepare questions to ask interviewer

**Day Of**
- Arrive 5 minutes early (or join 5 min early for video)
- Have water and clear mind
- Speak clearly and explain your reasoning

### During Scenario Questions

**Structure Your Answer (STAR method)**

1. **Situation** (2 min): What's the problem?
   - "We have 10K to 100K users, global audience, 99.9% uptime needed"

2. **Task** (1 min): What's your goal?
   - "Design scalable, cost-effective infrastructure"

3. **Action** (10 min): How would you solve it?
   - Design architecture layers (CDN → LB → Compute → DB → Cache)
   - Explain each choice (why GKE vs App Engine vs Cloud Run)
   - Address requirements (scalability, cost, high availability)

4. **Result** (2 min): How do you verify?
   - Monitoring and alerting
   - Load testing
   - Disaster recovery drills

**Tips During Answer**

✓ **Think out loud**: "I would start with X because... Then add Y for..."
✓ **Draw while talking**: Whiteboard diagrams help explain faster
✓ **Make assumptions explicit**: "Assuming 10K concurrent users, global audience"
✓ **Mention trade-offs**: "We could also use Cloud Run, but GKE gives us more control"
✓ **Include cost**: "This solution costs about $1-2K/month"
✓ **Mention monitoring**: "We'd need alerts for CPU > 80%, error rate > 1%"

✗ **Avoid**:
- Getting stuck on details (cloud storage bucket naming)
- Saying "I don't know" (say "I would investigate first")
- Over-engineering (don't design for 1M users if requirement is 10K)
- Forgetting about costs
- No high availability / disaster recovery plan

### Interview Questions You Might Get

**Basic (Warm-up questions)**
- "What are the three main compute options in GCP?"
- "Difference between App Engine and Cloud Run?"
- "What is a GKE cluster?"

**Intermediate**
- "How do you scale a Kubernetes deployment?"
- "What's the difference between strong and eventual consistency?"
- "Design a multi-region failover strategy"

**Advanced**
- "How would you debug a slow microservice?"
- "Design a system that saves $500K per year"
- "How would you handle 1000x traffic spike?"

### Red Flags to Avoid

❌ **Single point of failure** - no load balancer, no database replication
❌ **Exposing databases** - databases should never be directly accessed from internet
❌ **No monitoring** - if something breaks, you won't know
❌ **No cost consideration** - $100K/month solution to $1K/month problem
❌ **Manual deployments** - everything should be automated with IaC
❌ **No backups** - data loss scenarios should be addressed
❌ **No rate limiting** - DDoS or runaway client will bring down system

### Green Flags That Impress

✅ **Global distribution** - you mention Cloud CDN, multiple regions
✅ **Automated scaling** - HPA for Kubernetes, auto-scaling groups
✅ **Zero-downtime deployments** - rolling updates, blue-green deployments
✅ **Proper security** - VPCs, firewalls, IAM roles, no hardcoded secrets
✅ **Infrastructure as Code** - Terraform for reproducibility
✅ **Comprehensive monitoring** - dashboards, alerts, SLOs/SLIs
✅ **Disaster recovery plan** - RTO/RPO, failover strategy, DR drills
✅ **Cost optimization** - Committed Use Discounts, resource right-sizing

### Common Interview Mistakes

**Mistake 1: Overengineering**
- Designing multi-region, 99.99% uptime when requirement is 99% uptime
- Using GKE for simple blog application (should use Cloud Run)
- Adding every possible service when simpler solution exists

**Mistake 2: Ignoring Requirements**
- Designing globally when users are only in US
- Choosing eventual consistency for financial transactions
- Using preemptible VMs for critical services

**Mistake 3: No Cost Discussion**
- Forgetting to mention monthly cost
- Not discussing cost optimization options
- Using most expensive services without justification

**Mistake 4: Incomplete Disaster Recovery**
- Single region architecture (single point of failure)
- No backup strategy
- No data loss/RTO/RPO considerations

**Mistake 5: Forgetting Security**
- Exposing databases to internet
- Hardcoding API keys or passwords
- No encryption, no audit logging
- Not using IAM principle of least privilege

### After Interview

**Follow-up (if asked for more detail)**
- Send well-structured email summarizing your architecture
- Include ASCII diagrams or links to draw.io diagram
- Mention cost breakdown
- Mention trade-offs you considered

**If They Ask "What else?"**
- "We should add dedicated alerting for database slow queries"
- "Implement blue-green deployments for zero-downtime updates"
- "Add distributed tracing (Datadog, New Relic) for debugging"
- "Use Terraform for infrastructure as code"
- "Implement chaos engineering to test failure scenarios"

---

## Summary: Key Interview Takeaways

```
GCP Architecture Interview = Ask Good Questions → Understand Requirements → Design Layers → Address Non-Functional Requirements → Discuss Trade-offs → Estimate Costs

✓ Always include:
  ├─ Load balancer (no direct access to backends)
  ├─ Auto-scaling (horizontal > vertical)
  ├─ Database replication (HA + DR)
  ├─ Caching layer (reduce database load)
  ├─ Monitoring & alerting (know when things break)
  └─ Cost estimate ($X/month)

✓ Mention these phrases:
  ├─ "Multi-zone for high availability"
  ├─ "Multi-region for disaster recovery"
  ├─ "Auto-scaling based on CPU/memory"
  ├─ "Infrastructure as Code (Terraform)"
  ├─ "Zero-downtime deployments"
  ├─ "Monitoring with metrics and alerts"
  └─ "This solution costs ~$X/month"

✓ Common services for interviews:
  ├─ CDN + Load Balancing: Global HTTPS LB + Cloud CDN
  ├─ Compute: GKE (preferred), Cloud Run (serverless), App Engine (PaaS)
  ├─ Database: Cloud SQL (ACID), Bigtable (scale), BigQuery (analytics)
  ├─ Cache: Memorystore (Redis)
  ├─ Async: Cloud Pub/Sub
  └─ IaC: Terraform
```


